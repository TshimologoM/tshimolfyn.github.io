{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "16OAefs0PeZhtQ1AXosINRa7xWtbTvYzw",
      "authorship_tag": "ABX9TyNNa42RSdGuGO5nnJtjyQoK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TshimologoM/tshimolfyn.github.io/blob/main/trade.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6JpZMrFKOrR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"end_to_end_project\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ],
      "metadata": {
        "id": "K_Bi707qMAkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip3 uninstall statsmodels\n",
        "# !pip3 install numpy scipy patsy pandas\n",
        "# !pip3 install statsmodels\n",
        "!pip install pmdarima\n",
        "# !pip install --upgrade --no-deps statsmodels\n",
        "# !pip install tensorflow==2.1.0"
      ],
      "metadata": {
        "id": "z37Tas5c-B1o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d74282f-04fe-4518-989e-ef5bb5fb544b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pmdarima\n",
            "  Downloading pmdarima-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.3.1)\n",
            "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (0.29.36)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.22.4)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.10.1)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (0.13.5)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.26.16)\n",
            "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pmdarima) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pmdarima) (2022.7.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->pmdarima) (3.1.0)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13.2->pmdarima) (0.5.3)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13.2->pmdarima) (23.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.2->statsmodels>=0.13.2->pmdarima) (1.16.0)\n",
            "Installing collected packages: pmdarima\n",
            "Successfully installed pmdarima-2.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "fTh5E1Q3Ag4T",
        "outputId": "497c41e9-76bb-4d72-8784-52a737bcddee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3cb2832d-b5a5-4501-86ec-371f14ee7695\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3cb2832d-b5a5-4501-86ec-371f14ee7695\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Exp_Imp_Trade.csv to Exp_Imp_Trade.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('Exp_Imp_Trade.csv')\n",
        "\n",
        "# df.tail()"
      ],
      "metadata": {
        "id": "nANxHbJNMCeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfc=df.copy()\n",
        "# choose countries to use\n",
        "df1=dfc[((df[\"Country Name\"]== \"United States\")|(df[\"Country Name\"]== \"China\")|(df[\"Country Name\"]== \"Japan\")\n",
        "       |(df[\"Country Name\"]== \"Germany\")|(df[\"Country Name\"]== \"United Kingdom\")|(df[\"Country Name\"]== \"India\")\n",
        "       |(df[\"Country Name\"]== \"France\")|(df[\"Country Name\"]== \"Italy\")|(df[\"Country Name\"]== \"Canada\")|(df[\"Country Name\"]== \"Korea, Rep.\")\n",
        "       |(df[\"Country Name\"]== \"Russian Federation\")|(df[\"Country Name\"]== \"Brazil\")|(df[\"Country Name\"]== \"Australia\")|(df[\"Country Name\"]== \"Spain\")|(df[\"Country Name\"]== \"Mexico\"))]\n",
        "# drop unwanted columns\n",
        "df1=df1.drop(\"Country Code\",axis=1)\n",
        "df1=df1.drop(\"Indicator Code\",axis=1)\n",
        "df1=df1.drop(\"Indicator Name\",axis=1)\n",
        "df1=df1.drop(df1.loc[:, '1960':'1989'].columns, axis = 1)\n",
        "df1=df1.drop(\"2021\",axis=1)\n",
        "# reset and rename\n",
        "df1.reset_index(drop=True,inplace=True)\n",
        "df1.rename(columns = {'Country Name':'Country'}, inplace = True)\n",
        "df1['Country'] = df1['Country'].replace(['United Kingdom'],'UK')\n",
        "df1['Country'] = df1['Country'].replace(['United States'],'USA')\n",
        "df1['Country'] = df1['Country'].replace(['Korea, Rep.'],'Korea')\n",
        "df1['Country'] = df1['Country'].replace(['Russian Federation'],'Russia')\n",
        "# df1.head(15)"
      ],
      "metadata": {
        "id": "CgZA5G8LA09a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "melted_df = pd.melt(df1, id_vars=[\"Country\"], var_name=\"Year\", value_name=\"GDP\")\n",
        "gdp_melted_df=melted_df.copy()\n",
        "gdp_melted_df['Year']=pd.to_datetime(gdp_melted_df['Year'])\n",
        "df3=gdp_melted_df[['Year','GDP']].set_index('Year')\n",
        "gdp_melted_df['Year']=pd.to_datetime(gdp_melted_df['Year'])\n",
        "df2 = melted_df.pivot(index='Year', columns='Country', values='GDP')\n",
        "# display(melted_df.head(15))\n",
        "# display(gdp_melted_df.head(15))\n",
        "# display(df2.tail(15))\n",
        "# display(df3.tail(15))"
      ],
      "metadata": {
        "id": "aDTfGBQlBCFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df2.reset_index()\n",
        "df2['Year'] = pd.to_datetime(df2['Year'])\n",
        "df2 = df2.set_index('Year')\n",
        "# df2.head()"
      ],
      "metadata": {
        "id": "mzizVijXDjb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total num of missing values:\")\n",
        "print(df2.isna().sum())\n",
        "print(\"\")\n",
        "# df2.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqswOWnxIay9",
        "outputId": "c37bff37-ebfa-4088-81f8-e06f6bd6b63e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total num of missing values:\n",
            "Country\n",
            "Australia    0\n",
            "Brazil       0\n",
            "Canada       0\n",
            "China        0\n",
            "France       0\n",
            "Germany      0\n",
            "India        0\n",
            "Italy        0\n",
            "Japan        0\n",
            "Korea        0\n",
            "Mexico       0\n",
            "Russia       0\n",
            "Spain        0\n",
            "UK           0\n",
            "USA          0\n",
            "dtype: int64\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.style.use('fivethirtyeight')\n",
        "# plt.rcParams.update({'figure.figsize':(24,12), 'figure.dpi':100})\n",
        "\n",
        "# fig, ((ax1, ax2, ax3, ax4), (ax5, ax6, ax7, ax8),\n",
        "#       (ax9, ax10, ax11, ax12), (ax13, ax14, ax15, ax16)) = plt.subplots(4, 4)\n",
        "# # fig.suptitle(\"Country's government expenditure trend plot\")\n",
        "\n",
        "# ax1.plot(df2[\"Australia\"])\n",
        "# ax1.set_title('Australia')\n",
        "# ax2.plot(df2[\"Brazil\"])\n",
        "# ax2.set_title('Brazil')\n",
        "# ax3.plot(df2[\"Canada\"])\n",
        "# ax3.set_title('Canada')\n",
        "# ax4.plot(df2[\"China\"])\n",
        "# ax4.set_title('China')\n",
        "# ax5.plot(df2[\"France\"])\n",
        "# ax5.set_title('France')\n",
        "# ax6.plot(df2[\"Germany\"])\n",
        "# ax6.set_title('Germany')\n",
        "# ax7.plot(df2[\"India\"])\n",
        "# ax7.set_title('India')\n",
        "# ax8.plot(df2[\"Italy\"])\n",
        "# ax8.set_title('Italy')\n",
        "# ax9.plot(df2[\"Japan\"])\n",
        "# ax9.set_title('Japan')\n",
        "# ax10.plot(df2[\"Korea\"])\n",
        "# ax10.set_title('Korea')\n",
        "# ax11.plot(df2[\"Mexico\"])\n",
        "# ax11.set_title('Mexico')\n",
        "# ax12.plot(df2[\"Russia\"])\n",
        "# ax12.set_title('Russia')\n",
        "# ax13.plot(df2[\"Spain\"])\n",
        "# ax13.set_title('Spain')\n",
        "# ax14.plot(df2[\"UK\"])\n",
        "# ax14.set_title('UK')\n",
        "# ax15.plot(df2[\"USA\"])\n",
        "# ax15.set_title('USA')\n",
        "# fig.delaxes(ax16)\n",
        "\n",
        "# plt.tight_layout()\n",
        "# fig.subplots_adjust(top=1.2)\n",
        "# for ax in fig.get_axes():\n",
        "#     ax.set(xlabel='Year', ylabel='Values')\n",
        "\n",
        "# # Hide x labels and tick labels for top plots and y ticks for right plots.\n",
        "# # for ax in fig.get_axes():\n",
        "# #     ax.label_outer()\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "v6v9FItzRNHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df2.describe().T"
      ],
      "metadata": {
        "id": "XX4RsNWt1bNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SVR\n",
        "import matplotlib.dates as mdates\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVR"
      ],
      "metadata": {
        "id": "bs8uNsdT8tgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df2.head()\n",
        "# dfi=df2.iloc[:,2]\n",
        "# dfi\n",
        "# dfis=dfi.values\n",
        "# dfis\n",
        "# df2len=len(df2.columns)\n",
        "# df2len"
      ],
      "metadata": {
        "id": "YkqEHppRr9wA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2len=len(df2.columns)\n",
        "i=0\n",
        "\n",
        "testvals=[]\n",
        "predicted=[]\n",
        "\n",
        "mapeList=[]\n",
        "rmseList=[]\n",
        "maeList=[]\n",
        "\n",
        "while (i <= df2len-1):\n",
        "    dfi=df2.iloc[:,i]\n",
        "    train=dfi[:25]\n",
        "    test=dfi[-6:]\n",
        "    # display(train.head())\n",
        "    # display(train.info())\n",
        "    # display(test.head())\n",
        "    # print(test.info())\n",
        "\n",
        "    # change the dates into ints for training\n",
        "    dates_df = train.copy() #!!!!!!!!!!!!!!!!\n",
        "    dates_df = dates_df.reset_index()\n",
        "\n",
        "    # Store the original dates for plotting the predicitons\n",
        "    org_dates = dates_df['Year']\n",
        "\n",
        "    # convert to ints\n",
        "    dates_df['Year'] = dates_df['Year'].map(mdates.date2num)\n",
        "\n",
        "    dates_df.head()\n",
        "    ####\n",
        "    # change the dates into ints for testing\n",
        "    dates_test_df = test.copy() #!!!!!!!!!!!!!!!!\n",
        "    dates_test_df = dates_test_df.reset_index()\n",
        "\n",
        "    # Store the original dates for plotting the predicitons\n",
        "    org_test_dates = dates_test_df['Year']\n",
        "\n",
        "    # convert to ints\n",
        "    dates_test_df['Year'] = dates_test_df['Year'].map(mdates.date2num)\n",
        "\n",
        "    # print(dates_test_df.head())\n",
        "    # creating test and train for svr fitting\n",
        "    X_train=dates_df['Year']\n",
        "    # d=dates_df.iloc[:,1]\n",
        "    # print(d)\n",
        "    y_train=dates_df.iloc[:,1]\n",
        "    X_test=dates_test_df['Year']\n",
        "    y_test=dates_test_df.iloc[:,1]\n",
        "\n",
        "    # Feature Scaling\n",
        "    sc_X = StandardScaler()\n",
        "    sc_y = StandardScaler()\n",
        "    X_train = sc_X.fit_transform(X_train.values.reshape(-1, 1))\n",
        "    y_train = sc_y.fit_transform(y_train.values.reshape(-1, 1))\n",
        "\n",
        "    # Fitting SVR to the dataset\n",
        "    regressor = SVR(kernel = 'rbf')\n",
        "    regressor.fit(X_train, y_train)\n",
        "    # Predicting a new result\n",
        "    y_pred= regressor.predict(sc_X.transform(X_test.values.reshape(-1, 1)))\n",
        "    dim2y_pred = sc_y.inverse_transform(y_pred.reshape(-1, 1))\n",
        "\n",
        "    testvals.append(y_test)\n",
        "    predicted.append(dim2y_pred)\n",
        "    # print(\"test values\")\n",
        "    # display(y_test)\n",
        "    # print(\"predicted values\")\n",
        "    # display(y_pred)\n",
        "\n",
        "    mape= np.mean(np.abs(y_pred - y_test)/np.abs(y_test))\n",
        "    rmse= np.mean((y_pred-y_test)**2)**.5\n",
        "    d= y_pred-y_test.values\n",
        "    mae= np.mean(abs(d))\n",
        "\n",
        "    # print(\"MAPE: \", mape)\n",
        "    # print(\"RMSE: \", rmse)\n",
        "    # print(\"MAE: \", mae)\n",
        "\n",
        "    mapeList.append(mape)\n",
        "    rmseList.append(rmse)\n",
        "    maeList.append(mae)\n",
        "\n",
        "    i+=1\n",
        "\n",
        "svr_fit=pd.DataFrame({'exact' : testvals,\n",
        "                    'predicted': predicted})\n",
        "svr_fit.to_csv('Trade_svr_preds.csv')\n",
        "\n",
        "svr_metrics=pd.DataFrame({'MAPE' : mapeList,\n",
        "                    'RMSE': rmseList,\n",
        "                    'MAE': maeList})\n",
        "svr_metrics.to_csv('Trade_svr_metrics.csv')"
      ],
      "metadata": {
        "id": "RqglczrcQzhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip3 uninstall statsmodels\n",
        "# !pip3 install numpy scipy patsy pandas\n",
        "# !pip3 install statsmodels\n",
        "!pip install pmdarima\n",
        "# !pip install --upgrade --no-deps statsmodels"
      ],
      "metadata": {
        "id": "MO3X7rOccrOU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ba5f67b-a8c7-45e5-83ae-8f2aa50483bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pmdarima in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.3.1)\n",
            "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (0.29.36)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.22.4)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.10.1)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (0.13.5)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.26.16)\n",
            "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pmdarima) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pmdarima) (2022.7.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->pmdarima) (3.1.0)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13.2->pmdarima) (0.5.3)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13.2->pmdarima) (23.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.2->statsmodels>=0.13.2->pmdarima) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "ARIMA\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZLS3EKksqFsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from statsmodels.tsa.arima_model import ARIMA\n",
        "import pmdarima as pm\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "df2len=len(df2.columns)\n",
        "i=0\n",
        "\n",
        "predicted=[]\n",
        "\n",
        "mapeList=[]\n",
        "rmseList=[]\n",
        "maeList=[]\n",
        "\n",
        "while (i <= df2len-1):\n",
        "    series=df2.iloc[:,i]\n",
        "\n",
        "    model = pm.auto_arima(series, start_p=1, start_q=1,\n",
        "                          test='adf',       # use adftest to find optimal 'd'\n",
        "                          max_p=3, max_q=3, # maximum p and q\n",
        "                          m=1,              # frequency of series\n",
        "                          d=None,           # let model determine 'd'\n",
        "                          seasonal=False,   # No Seasonality\n",
        "                          start_P=0,\n",
        "                          D=0,\n",
        "                          trace=True,\n",
        "                          error_action='ignore',\n",
        "                          suppress_warnings=True,\n",
        "                          stepwise=True)\n",
        "\n",
        "    #     print(model.summary())\n",
        "    values = series.values\n",
        "    n_periods = len(series)\n",
        "    fc, confint = model.predict(n_periods=n_periods, return_conf_int=True)\n",
        "\n",
        "    predicted.append(fc)\n",
        "    # print(\"predicted values\")\n",
        "    # display(fc)\n",
        "\n",
        "    mape= np.mean(np.abs(fc - values)/np.abs(values))\n",
        "    rmse= np.mean((fc-values)**2)**.5\n",
        "    d= fc-values\n",
        "    mae= np.mean(abs(d))\n",
        "\n",
        "    # print(\"MAPE: \", mape)\n",
        "    # print(\"RMSE: \", rmse)\n",
        "    # print(\"MAE: \", mae)\n",
        "\n",
        "    mapeList.append(mape)\n",
        "    rmseList.append(rmse)\n",
        "    maeList.append(mae)\n",
        "\n",
        "    i+=1\n",
        "\n",
        "asmr_fit=pd.DataFrame({'predicted': predicted})\n",
        "asmr_fit.to_csv('Trade_asmr_preds.csv')\n",
        "\n",
        "asmr_metrics=pd.DataFrame({'MAPE' : mapeList,\n",
        "                    'RMSE': rmseList,\n",
        "                    'MAE': maeList})\n",
        "asmr_metrics.to_csv('Trade_asmr_metrics.csv')"
      ],
      "metadata": {
        "id": "mrW7kTLQZ3iG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77a792d0-00f8-47d7-d473-cad631ea6024"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing stepwise search to minimize aic\n",
            " ARIMA(1,1,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.26 sec\n",
            " ARIMA(0,1,0)(0,0,0)[0] intercept   : AIC=128.927, Time=0.01 sec\n",
            " ARIMA(1,1,0)(0,0,0)[0] intercept   : AIC=129.759, Time=0.03 sec\n",
            " ARIMA(0,1,1)(0,0,0)[0] intercept   : AIC=129.218, Time=0.02 sec\n",
            " ARIMA(0,1,0)(0,0,0)[0]             : AIC=128.149, Time=0.01 sec\n",
            "\n",
            "Best model:  ARIMA(0,1,0)(0,0,0)[0]          \n",
            "Total fit time: 0.365 seconds\n",
            "Performing stepwise search to minimize aic\n",
            " ARIMA(1,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.24 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0] intercept   : AIC=151.972, Time=0.01 sec\n",
            " ARIMA(1,2,0)(0,0,0)[0] intercept   : AIC=141.232, Time=0.03 sec\n",
            " ARIMA(0,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.13 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0]             : AIC=149.995, Time=0.02 sec\n",
            " ARIMA(2,2,0)(0,0,0)[0] intercept   : AIC=141.337, Time=0.04 sec\n",
            " ARIMA(2,2,1)(0,0,0)[0] intercept   : AIC=143.334, Time=0.07 sec\n",
            " ARIMA(1,2,0)(0,0,0)[0]             : AIC=139.237, Time=0.02 sec\n",
            " ARIMA(2,2,0)(0,0,0)[0]             : AIC=139.354, Time=0.02 sec\n",
            " ARIMA(1,2,1)(0,0,0)[0]             : AIC=inf, Time=0.11 sec\n",
            " ARIMA(0,2,1)(0,0,0)[0]             : AIC=inf, Time=0.07 sec\n",
            " ARIMA(2,2,1)(0,0,0)[0]             : AIC=141.352, Time=0.04 sec\n",
            "\n",
            "Best model:  ARIMA(1,2,0)(0,0,0)[0]          \n",
            "Total fit time: 0.810 seconds\n",
            "Performing stepwise search to minimize aic\n",
            " ARIMA(1,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.18 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0] intercept   : AIC=161.640, Time=0.01 sec\n",
            " ARIMA(1,2,0)(0,0,0)[0] intercept   : AIC=158.440, Time=0.03 sec\n",
            " ARIMA(0,2,1)(0,0,0)[0] intercept   : AIC=155.352, Time=0.03 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0]             : AIC=159.678, Time=0.03 sec\n",
            " ARIMA(0,2,2)(0,0,0)[0] intercept   : AIC=inf, Time=0.14 sec\n",
            " ARIMA(1,2,2)(0,0,0)[0] intercept   : AIC=inf, Time=0.21 sec\n",
            " ARIMA(0,2,1)(0,0,0)[0]             : AIC=153.900, Time=0.02 sec\n",
            " ARIMA(1,2,1)(0,0,0)[0]             : AIC=155.817, Time=0.03 sec\n",
            " ARIMA(0,2,2)(0,0,0)[0]             : AIC=155.831, Time=0.03 sec\n",
            " ARIMA(1,2,0)(0,0,0)[0]             : AIC=156.543, Time=0.02 sec\n",
            " ARIMA(1,2,2)(0,0,0)[0]             : AIC=157.200, Time=0.10 sec\n",
            "\n",
            "Best model:  ARIMA(0,2,1)(0,0,0)[0]          \n",
            "Total fit time: 0.853 seconds\n",
            "Performing stepwise search to minimize aic\n",
            " ARIMA(1,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.16 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0] intercept   : AIC=185.407, Time=0.01 sec\n",
            " ARIMA(1,2,0)(0,0,0)[0] intercept   : AIC=181.510, Time=0.04 sec\n",
            " ARIMA(0,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.15 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0]             : AIC=183.418, Time=0.01 sec\n",
            " ARIMA(2,2,0)(0,0,0)[0] intercept   : AIC=180.583, Time=0.05 sec\n",
            " ARIMA(3,2,0)(0,0,0)[0] intercept   : AIC=182.275, Time=0.06 sec\n",
            " ARIMA(2,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.15 sec\n",
            " ARIMA(3,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.16 sec\n",
            " ARIMA(2,2,0)(0,0,0)[0]             : AIC=178.626, Time=0.03 sec\n",
            " ARIMA(1,2,0)(0,0,0)[0]             : AIC=179.542, Time=0.03 sec\n",
            " ARIMA(3,2,0)(0,0,0)[0]             : AIC=180.340, Time=0.04 sec\n",
            " ARIMA(2,2,1)(0,0,0)[0]             : AIC=177.846, Time=0.05 sec\n",
            " ARIMA(1,2,1)(0,0,0)[0]             : AIC=175.933, Time=0.03 sec\n",
            " ARIMA(0,2,1)(0,0,0)[0]             : AIC=174.551, Time=0.02 sec\n",
            " ARIMA(0,2,2)(0,0,0)[0]             : AIC=176.003, Time=0.03 sec\n",
            " ARIMA(1,2,2)(0,0,0)[0]             : AIC=inf, Time=0.13 sec\n",
            "\n",
            "Best model:  ARIMA(0,2,1)(0,0,0)[0]          \n",
            "Total fit time: 1.192 seconds\n",
            "Performing stepwise search to minimize aic\n",
            " ARIMA(1,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.19 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0] intercept   : AIC=161.428, Time=0.01 sec\n",
            " ARIMA(1,2,0)(0,0,0)[0] intercept   : AIC=158.864, Time=0.03 sec\n",
            " ARIMA(0,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.12 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0]             : AIC=159.541, Time=0.02 sec\n",
            " ARIMA(2,2,0)(0,0,0)[0] intercept   : AIC=157.885, Time=0.04 sec\n",
            " ARIMA(3,2,0)(0,0,0)[0] intercept   : AIC=157.769, Time=0.06 sec\n",
            " ARIMA(3,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.19 sec\n",
            " ARIMA(2,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.12 sec\n",
            " ARIMA(3,2,0)(0,0,0)[0]             : AIC=155.846, Time=0.03 sec\n",
            " ARIMA(2,2,0)(0,0,0)[0]             : AIC=155.976, Time=0.02 sec\n",
            " ARIMA(3,2,1)(0,0,0)[0]             : AIC=inf, Time=0.20 sec\n",
            " ARIMA(2,2,1)(0,0,0)[0]             : AIC=inf, Time=0.27 sec\n",
            "\n",
            "Best model:  ARIMA(3,2,0)(0,0,0)[0]          \n",
            "Total fit time: 1.319 seconds\n",
            "Performing stepwise search to minimize aic\n",
            " ARIMA(1,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.32 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0] intercept   : AIC=183.123, Time=0.03 sec\n",
            " ARIMA(1,2,0)(0,0,0)[0] intercept   : AIC=180.435, Time=0.06 sec\n",
            " ARIMA(0,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.29 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0]             : AIC=181.210, Time=0.03 sec\n",
            " ARIMA(2,2,0)(0,0,0)[0] intercept   : AIC=179.789, Time=0.09 sec\n",
            " ARIMA(3,2,0)(0,0,0)[0] intercept   : AIC=177.945, Time=0.13 sec\n",
            " ARIMA(3,2,1)(0,0,0)[0] intercept   : AIC=175.554, Time=0.16 sec\n",
            " ARIMA(2,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.30 sec\n",
            " ARIMA(3,2,2)(0,0,0)[0] intercept   : AIC=inf, Time=0.54 sec\n",
            " ARIMA(2,2,2)(0,0,0)[0] intercept   : AIC=inf, Time=0.60 sec\n",
            " ARIMA(3,2,1)(0,0,0)[0]             : AIC=173.668, Time=0.13 sec\n",
            " ARIMA(2,2,1)(0,0,0)[0]             : AIC=172.902, Time=0.15 sec\n",
            " ARIMA(1,2,1)(0,0,0)[0]             : AIC=inf, Time=0.23 sec\n",
            " ARIMA(2,2,0)(0,0,0)[0]             : AIC=177.843, Time=0.07 sec\n",
            " ARIMA(2,2,2)(0,0,0)[0]             : AIC=inf, Time=0.44 sec\n",
            " ARIMA(1,2,0)(0,0,0)[0]             : AIC=178.520, Time=0.05 sec\n",
            " ARIMA(1,2,2)(0,0,0)[0]             : AIC=inf, Time=0.19 sec\n",
            " ARIMA(3,2,0)(0,0,0)[0]             : AIC=175.976, Time=0.03 sec\n",
            " ARIMA(3,2,2)(0,0,0)[0]             : AIC=inf, Time=0.23 sec\n",
            "\n",
            "Best model:  ARIMA(2,2,1)(0,0,0)[0]          \n",
            "Total fit time: 4.115 seconds\n",
            "Performing stepwise search to minimize aic\n",
            " ARIMA(1,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.20 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0] intercept   : AIC=176.125, Time=0.01 sec\n",
            " ARIMA(1,2,0)(0,0,0)[0] intercept   : AIC=169.067, Time=0.03 sec\n",
            " ARIMA(0,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.08 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0]             : AIC=174.145, Time=0.02 sec\n",
            " ARIMA(2,2,0)(0,0,0)[0] intercept   : AIC=169.383, Time=0.04 sec\n",
            " ARIMA(2,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.16 sec\n",
            " ARIMA(1,2,0)(0,0,0)[0]             : AIC=167.152, Time=0.02 sec\n",
            " ARIMA(2,2,0)(0,0,0)[0]             : AIC=167.483, Time=0.03 sec\n",
            " ARIMA(1,2,1)(0,0,0)[0]             : AIC=163.701, Time=0.04 sec\n",
            " ARIMA(0,2,1)(0,0,0)[0]             : AIC=161.703, Time=0.03 sec\n",
            " ARIMA(0,2,2)(0,0,0)[0]             : AIC=163.702, Time=0.03 sec\n",
            " ARIMA(1,2,2)(0,0,0)[0]             : AIC=165.344, Time=0.13 sec\n",
            "\n",
            "Best model:  ARIMA(0,2,1)(0,0,0)[0]          \n",
            "Total fit time: 0.848 seconds\n",
            "Performing stepwise search to minimize aic\n",
            " ARIMA(1,1,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.21 sec\n",
            " ARIMA(0,1,0)(0,0,0)[0] intercept   : AIC=156.845, Time=0.01 sec\n",
            " ARIMA(1,1,0)(0,0,0)[0] intercept   : AIC=158.452, Time=0.11 sec\n",
            " ARIMA(0,1,1)(0,0,0)[0] intercept   : AIC=155.592, Time=0.04 sec\n",
            " ARIMA(0,1,0)(0,0,0)[0]             : AIC=156.089, Time=0.02 sec\n",
            " ARIMA(0,1,2)(0,0,0)[0] intercept   : AIC=inf, Time=0.10 sec\n",
            " ARIMA(1,1,2)(0,0,0)[0] intercept   : AIC=inf, Time=0.19 sec\n",
            " ARIMA(0,1,1)(0,0,0)[0]             : AIC=157.967, Time=0.03 sec\n",
            "\n",
            "Best model:  ARIMA(0,1,1)(0,0,0)[0] intercept\n",
            "Total fit time: 0.718 seconds\n",
            "Performing stepwise search to minimize aic\n",
            " ARIMA(1,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.17 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0] intercept   : AIC=166.668, Time=0.02 sec\n",
            " ARIMA(1,2,0)(0,0,0)[0] intercept   : AIC=164.451, Time=0.03 sec\n",
            " ARIMA(0,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.07 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0]             : AIC=164.679, Time=0.02 sec\n",
            " ARIMA(2,2,0)(0,0,0)[0] intercept   : AIC=161.296, Time=0.04 sec\n",
            " ARIMA(3,2,0)(0,0,0)[0] intercept   : AIC=160.339, Time=0.06 sec\n",
            " ARIMA(3,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.22 sec\n",
            " ARIMA(2,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.14 sec\n",
            " ARIMA(3,2,0)(0,0,0)[0]             : AIC=158.340, Time=0.03 sec\n",
            " ARIMA(2,2,0)(0,0,0)[0]             : AIC=159.301, Time=0.03 sec\n",
            " ARIMA(3,2,1)(0,0,0)[0]             : AIC=inf, Time=0.14 sec\n",
            " ARIMA(2,2,1)(0,0,0)[0]             : AIC=inf, Time=0.12 sec\n",
            "\n",
            "Best model:  ARIMA(3,2,0)(0,0,0)[0]          \n",
            "Total fit time: 1.100 seconds\n",
            "Performing stepwise search to minimize aic\n",
            " ARIMA(1,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.10 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0] intercept   : AIC=221.268, Time=0.02 sec\n",
            " ARIMA(1,2,0)(0,0,0)[0] intercept   : AIC=215.602, Time=0.03 sec\n",
            " ARIMA(0,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.07 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0]             : AIC=219.279, Time=0.01 sec\n",
            " ARIMA(2,2,0)(0,0,0)[0] intercept   : AIC=212.126, Time=0.04 sec\n",
            " ARIMA(3,2,0)(0,0,0)[0] intercept   : AIC=212.241, Time=0.07 sec\n",
            " ARIMA(2,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.15 sec\n",
            " ARIMA(3,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.19 sec\n",
            " ARIMA(2,2,0)(0,0,0)[0]             : AIC=210.144, Time=0.03 sec\n",
            " ARIMA(1,2,0)(0,0,0)[0]             : AIC=213.622, Time=0.02 sec\n",
            " ARIMA(3,2,0)(0,0,0)[0]             : AIC=210.254, Time=0.04 sec\n",
            " ARIMA(2,2,1)(0,0,0)[0]             : AIC=208.723, Time=0.07 sec\n",
            " ARIMA(1,2,1)(0,0,0)[0]             : AIC=206.735, Time=0.07 sec\n",
            " ARIMA(0,2,1)(0,0,0)[0]             : AIC=204.736, Time=0.03 sec\n",
            " ARIMA(0,2,2)(0,0,0)[0]             : AIC=206.735, Time=0.04 sec\n",
            " ARIMA(1,2,2)(0,0,0)[0]             : AIC=208.550, Time=0.16 sec\n",
            "\n",
            "Best model:  ARIMA(0,2,1)(0,0,0)[0]          \n",
            "Total fit time: 1.153 seconds\n",
            "Performing stepwise search to minimize aic\n",
            " ARIMA(1,1,1)(0,0,0)[0] intercept   : AIC=174.203, Time=0.05 sec\n",
            " ARIMA(0,1,0)(0,0,0)[0] intercept   : AIC=172.455, Time=0.01 sec\n",
            " ARIMA(1,1,0)(0,0,0)[0] intercept   : AIC=173.809, Time=0.05 sec\n",
            " ARIMA(0,1,1)(0,0,0)[0] intercept   : AIC=172.937, Time=0.04 sec\n",
            " ARIMA(0,1,0)(0,0,0)[0]             : AIC=173.401, Time=0.01 sec\n",
            "\n",
            "Best model:  ARIMA(0,1,0)(0,0,0)[0] intercept\n",
            "Total fit time: 0.168 seconds\n",
            "Performing stepwise search to minimize aic\n",
            " ARIMA(1,0,1)(0,0,0)[0]             : AIC=261.776, Time=0.11 sec\n",
            " ARIMA(0,0,0)(0,0,0)[0]             : AIC=338.907, Time=0.01 sec\n",
            " ARIMA(1,0,0)(0,0,0)[0]             : AIC=272.297, Time=0.02 sec\n",
            " ARIMA(0,0,1)(0,0,0)[0]             : AIC=310.871, Time=0.03 sec\n",
            " ARIMA(2,0,1)(0,0,0)[0]             : AIC=263.587, Time=0.08 sec\n",
            " ARIMA(1,0,2)(0,0,0)[0]             : AIC=263.398, Time=0.08 sec\n",
            " ARIMA(0,0,2)(0,0,0)[0]             : AIC=300.064, Time=0.07 sec\n",
            " ARIMA(2,0,0)(0,0,0)[0]             : AIC=270.309, Time=0.04 sec\n",
            " ARIMA(2,0,2)(0,0,0)[0]             : AIC=263.797, Time=0.13 sec\n",
            " ARIMA(1,0,1)(0,0,0)[0] intercept   : AIC=257.122, Time=0.07 sec\n",
            " ARIMA(0,0,1)(0,0,0)[0] intercept   : AIC=254.644, Time=0.04 sec\n",
            " ARIMA(0,0,0)(0,0,0)[0] intercept   : AIC=253.134, Time=0.01 sec\n",
            " ARIMA(1,0,0)(0,0,0)[0] intercept   : AIC=254.831, Time=0.05 sec\n",
            "\n",
            "Best model:  ARIMA(0,0,0)(0,0,0)[0] intercept\n",
            "Total fit time: 0.765 seconds\n",
            "Performing stepwise search to minimize aic\n",
            " ARIMA(1,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.16 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0] intercept   : AIC=168.166, Time=0.01 sec\n",
            " ARIMA(1,2,0)(0,0,0)[0] intercept   : AIC=168.322, Time=0.03 sec\n",
            " ARIMA(0,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.09 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0]             : AIC=166.268, Time=0.02 sec\n",
            "\n",
            "Best model:  ARIMA(0,2,0)(0,0,0)[0]          \n",
            "Total fit time: 0.331 seconds\n",
            "Performing stepwise search to minimize aic\n",
            " ARIMA(1,1,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.19 sec\n",
            " ARIMA(0,1,0)(0,0,0)[0] intercept   : AIC=146.524, Time=0.01 sec\n",
            " ARIMA(1,1,0)(0,0,0)[0] intercept   : AIC=148.380, Time=0.02 sec\n",
            " ARIMA(0,1,1)(0,0,0)[0] intercept   : AIC=148.237, Time=0.03 sec\n",
            " ARIMA(0,1,0)(0,0,0)[0]             : AIC=144.873, Time=0.01 sec\n",
            "\n",
            "Best model:  ARIMA(0,1,0)(0,0,0)[0]          \n",
            "Total fit time: 0.284 seconds\n",
            "Performing stepwise search to minimize aic\n",
            " ARIMA(1,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.14 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0] intercept   : AIC=138.386, Time=0.01 sec\n",
            " ARIMA(1,2,0)(0,0,0)[0] intercept   : AIC=133.848, Time=0.03 sec\n",
            " ARIMA(0,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.08 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0]             : AIC=136.434, Time=0.01 sec\n",
            " ARIMA(2,2,0)(0,0,0)[0] intercept   : AIC=130.390, Time=0.03 sec\n",
            " ARIMA(3,2,0)(0,0,0)[0] intercept   : AIC=129.125, Time=0.05 sec\n",
            " ARIMA(3,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.22 sec\n",
            " ARIMA(2,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.18 sec\n",
            " ARIMA(3,2,0)(0,0,0)[0]             : AIC=127.295, Time=0.04 sec\n",
            " ARIMA(2,2,0)(0,0,0)[0]             : AIC=128.517, Time=0.02 sec\n",
            " ARIMA(3,2,1)(0,0,0)[0]             : AIC=125.271, Time=0.05 sec\n",
            " ARIMA(2,2,1)(0,0,0)[0]             : AIC=123.498, Time=0.04 sec\n",
            " ARIMA(1,2,1)(0,0,0)[0]             : AIC=122.802, Time=0.04 sec\n",
            " ARIMA(0,2,1)(0,0,0)[0]             : AIC=120.954, Time=0.03 sec\n",
            " ARIMA(0,2,2)(0,0,0)[0]             : AIC=122.664, Time=0.04 sec\n",
            " ARIMA(1,2,0)(0,0,0)[0]             : AIC=131.939, Time=0.02 sec\n",
            " ARIMA(1,2,2)(0,0,0)[0]             : AIC=124.952, Time=0.05 sec\n",
            "\n",
            "Best model:  ARIMA(0,2,1)(0,0,0)[0]          \n",
            "Total fit time: 1.117 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "ETS\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GJX7KNz-qSnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ETS\n",
        "from math import sqrt\n",
        "from statsmodels.tsa.api import ExponentialSmoothing\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error,mean_absolute_percentage_error, r2_score\n",
        "\n",
        "df2len=len(df2.columns)\n",
        "i=0\n",
        "\n",
        "predictedets=[]\n",
        "\n",
        "emapeList=[]\n",
        "ermseList=[]\n",
        "emaeList=[]\n",
        "edlist=[]\n",
        "\n",
        "while (i <= df2len-1):\n",
        "    dfi=df2.iloc[:,i]\n",
        "    train=dfi[:25]\n",
        "    test=dfi[-6:]\n",
        "\n",
        "    xlabel_name = 'Year'\n",
        "    ylabel_name = 'GDP'\n",
        "    # pre_name = 'GDP'\n",
        "    train_ETS = train.values#!!!!!!!!\n",
        "    test_ETS = test.values#!!!!!!!!\n",
        "\n",
        "    history = [x for x in train_ETS]\n",
        "    y_ets = test_ETS\n",
        "    # make first prediction\n",
        "    predictions=list()\n",
        "    for t in range(len(test)):#!!!!!!!!!!!!!!\n",
        "        model_ets = ExponentialSmoothing(history)\n",
        "        # model_fit = model.fit()\n",
        "        # model_fit = model.fit(smoothing_level=0.9,optimized=False)\n",
        "        model_fit = model_ets.fit(smoothing_level=0.44,smoothing_slope=0.011)\n",
        "        yhat = model_fit.forecast()[0]\n",
        "        predictions.append(yhat)\n",
        "        obs = y_ets[t]\n",
        "        history.append(obs)\n",
        "\n",
        "        emape= np.mean(np.abs(yhat - y_ets)/np.abs(y_ets))\n",
        "        ermse= np.mean((yhat-y_ets)**2)**.5\n",
        "        ed= yhat-y_ets\n",
        "        emae= np.mean(abs(ed))\n",
        "\n",
        "    emapeList.append(emape)\n",
        "    ermseList.append(ermse)\n",
        "    emaeList.append(emae)\n",
        "    edlist.append(ed)\n",
        "\n",
        "        # display(predictions)\n",
        "\n",
        "    predictedets.append(predictions)\n",
        "\n",
        "\n",
        "\n",
        "    # print(\"MAPE: \", emape)\n",
        "    # print(\"RMSE: \", ermse)\n",
        "    # print(\"MAE: \", emae)\n",
        "    # print(\"d\", ed)\n",
        "\n",
        "    i+=1\n",
        "\n",
        "ets_fit=pd.DataFrame({'predicted': predictedets})\n",
        "ets_fit.to_csv('Trade_ets_preds.csv')\n",
        "\n",
        "ets_metrics=pd.DataFrame({'MAPE' : emapeList,\n",
        "                    'RMSE': ermseList,\n",
        "                    'MAE': emaeList})\n",
        "ets_metrics.to_csv('Trade_ets_metrics.csv')"
      ],
      "metadata": {
        "id": "4FawPofonz3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "XGBOOST\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "OVkrggmix87Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost\n",
        "print(\"xgboost\", xgboost.__version__)"
      ],
      "metadata": {
        "id": "_Uk8rNilux6v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8754bfe5-aa04-482f-f062-2d111f3d9103"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xgboost 1.7.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import asarray\n",
        "from pandas import concat\n",
        "from pandas import DataFrame\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from xgboost import XGBRegressor"
      ],
      "metadata": {
        "id": "6De-eSGdypou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transform a time series dataset into a supervised learning dataset\n",
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "\tn_vars = 1 if type(data) is list else data.shape[0]\n",
        "\tdf = DataFrame(data)\n",
        "\tcols = list()\n",
        "\t# input sequence (t-n, ... t-1)\n",
        "\tfor i in range(n_in, 0, -1):\n",
        "\t\tcols.append(df.shift(i))\n",
        "\t# forecast sequence (t, t+1, ... t+n)\n",
        "\tfor i in range(0, n_out):\n",
        "\t\tcols.append(df.shift(-i))\n",
        "\t# put it all together\n",
        "\tagg = concat(cols, axis=1)\n",
        "\t# drop rows with NaN values\n",
        "\tif dropnan:\n",
        "\t\tagg.dropna(inplace=True)\n",
        "\treturn agg.values\n",
        "#\n",
        "# split a univariate dataset into train/test sets\n",
        "def train_test_split(data, n_test):\n",
        "\treturn data[:-n_test, :], data[-n_test:, :]\n",
        "\n",
        "# fit an xgboost model and make a one step prediction\n",
        "def xgboost_forecast(train, testX):\n",
        "\t# transform list into array\n",
        "\ttrain = asarray(train)\n",
        "\t# split into input and output columns\n",
        "\ttrainX, trainy = train[:, :-1], train[:, -1]\n",
        "\t# fit model\n",
        "\tmodel = XGBRegressor(objective='reg:squarederror', n_estimators=1000)\n",
        "\tmodel.fit(trainX, trainy)\n",
        "\t# make a one-step prediction\n",
        "\tyhat = model.predict(asarray([testX]))\n",
        "\treturn yhat[0]"
      ],
      "metadata": {
        "id": "Cw-judQHyx8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dataset\n",
        "\n",
        "dataset_col =df2\n",
        "df2len=len(df2.columns)\n",
        "i=0\n",
        "\n",
        "predictedxg=[]\n",
        "\n",
        "xmapeList=[]\n",
        "xrmseList=[]\n",
        "xmaeList=[]\n",
        "\n",
        "while (i <= df2len-1):\n",
        "    series=df2.iloc[:,i]\n",
        "    values = series.values\n",
        "    # transform the time series data into supervised learning\n",
        "    data = series_to_supervised(values, n_in=6)\n",
        "\n",
        "# walk-forward validation for univariate data\n",
        "    predictions = list()\n",
        "    # split dataset\n",
        "    train, test = train_test_split(data, 6)\n",
        "    # seed history with training dataset\n",
        "    history_xg = [x for x in train]\n",
        "    # step over each time-step in the test set\n",
        "    for j in range(len(test)):\n",
        "      # split test row into input and output columns\n",
        "      testX, testy = test[j, :-1], test[j, -1]\n",
        "      # fit model on history and make a prediction\n",
        "      yhat = xgboost_forecast(history_xg, testX)\n",
        "      # store forecast in list of predictions\n",
        "      predictions.append(yhat)\n",
        "      # add actual observation to history for the next loop\n",
        "      history_xg.append(test[j])\n",
        "\n",
        "      mape= np.mean(np.abs(yhat - testy)/np.abs(testy))\n",
        "      rmse= np.mean((yhat-testy)**2)**.5\n",
        "      d= yhat-testy\n",
        "      mae= np.mean(abs(d))\n",
        "\n",
        "    xmapeList.append(mape)\n",
        "    xrmseList.append(rmse)\n",
        "    xmaeList.append(mae)\n",
        "\n",
        "    predictedxg.append(predictions)\n",
        "\n",
        "    # print(\"MAPE: \", mape)\n",
        "    # print(\"RMSE: \", rmse)\n",
        "    # print(\"MAE: \", mae)\n",
        "    # print(\"d:\",d)\n",
        "\n",
        "    i+=1\n",
        "\n",
        "xgb_fit=pd.DataFrame({'predicted': predictedxg})\n",
        "xgb_fit.to_csv('Trade_xgb_preds.csv')\n",
        "\n",
        "xgb_metrics=pd.DataFrame({'MAPE' : xmapeList,\n",
        "                    'RMSE': xrmseList,\n",
        "                    'MAE': xmaeList})\n",
        "xgb_metrics.to_csv('Trade_xgb_metrics.csv')"
      ],
      "metadata": {
        "id": "awkgZrI5y4Dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "NEURAL NETWORKS\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "H8D3CxnmDVSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !sudo pip3 install keras"
      ],
      "metadata": {
        "id": "wJsTXwjxFSNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.client import device_lib\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from tensorflow.python.keras.engine.sequential import Sequential\n",
        "from tensorflow.python.keras import layers, callbacks\n",
        "from tensorflow.python.keras.layers import Dense, Conv2D, LSTM, Dropout, GRU, Flatten, Dropout, SpatialDropout1D, MaxPooling2D,CuDNNGRU\n",
        "from keras import optimizers\n",
        "# print(device_lib.list_local_devices())\n",
        "# print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
      ],
      "metadata": {
        "id": "8BPThvYfEOC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.random.set_seed(1234)"
      ],
      "metadata": {
        "id": "gLQhWFnqzAsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2i=df2.iloc[:,0]\n",
        "# df2i=df2i.drop('Year')\n",
        "df2i=df2i.reset_index(drop=True)\n",
        "df2i.head()"
      ],
      "metadata": {
        "id": "PURujA5fCfHT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "919c614d-0ada-446d-b9b4-795b888556bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    32.163364\n",
              "1    32.196863\n",
              "2    33.049085\n",
              "3    35.400412\n",
              "4    36.458731\n",
              "Name: Australia, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create input dataset\n",
        "def create_dataset (X, look_back = 1):\n",
        "    Xs, ys = [], []\n",
        "\n",
        "    for i in range(len(X)-look_back):\n",
        "        v = X[i:i+look_back]\n",
        "        Xs.append(v)\n",
        "        ys.append(X[i+look_back])\n",
        "\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "# split a univariate sequence into samples\n",
        "def split_sequence(sequence, n_steps):\n",
        "\tX, y = list(), list()\n",
        "\tfor i in range(len(sequence)):\n",
        "\t\t# find the end of this pattern (n_steps is similar to look_back)\n",
        "\t\tend_ix = i + n_steps\n",
        "\t\t# check if we are beyond the sequence\n",
        "\t\tif end_ix > len(sequence)-1:\n",
        "\t\t\tbreak\n",
        "\t\t# gather input and output parts of the pattern\n",
        "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
        "\t\tX.append(seq_x)\n",
        "\t\ty.append(seq_y)\n",
        "\treturn array(X), array(y)"
      ],
      "metadata": {
        "id": "an-IQ8L9Q6We"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "GRU\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "3Nnib079kGTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2len=len(df2.columns)\n",
        "i=0\n",
        "\n",
        "testvals=[]\n",
        "predicted=[]\n",
        "\n",
        "mapeList=[]\n",
        "rmseList=[]\n",
        "maeList=[]\n",
        "\n",
        "while (i <= df2len-1):\n",
        "    dfi=df2.iloc[:,i]\n",
        "    dfi=dfi.reset_index(drop=True)\n",
        "    train=dfi[:24]\n",
        "    test=dfi[-7:]\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    sc_train = scaler.fit_transform(train.values.reshape(-1, 1))\n",
        "    train_scaled = scaler.transform(train.values.reshape(-1, 1))\n",
        "    sc_test = scaler.fit_transform(test.values.reshape(-1,1))\n",
        "    test_scaled = scaler.transform(test.values.reshape(-1,1))\n",
        "\n",
        "    LOOK_BACK = 1\n",
        "    X_train, y_train = create_dataset(train_scaled,LOOK_BACK)\n",
        "    X_test, y_test = create_dataset(test_scaled,LOOK_BACK)\n",
        "    # Print data shape\n",
        "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "    # print(\"X_train.shape:\", X_train.shape)\n",
        "    # print(\"y_train.shape:\", y_train.shape)\n",
        "    # print(\"X_test.shape:\", X_test.shape)\n",
        "    # print(\"y_test.shape:\", y_test.shape)\n",
        "\n",
        "\n",
        "    model = Sequential()\n",
        "    # Input layer\n",
        "    model.add(GRU (units = 200, return_sequences = True,\n",
        "              input_shape = [X_train.shape[1], X_train.shape[2]]))\n",
        "    model.add(Dropout(0.2))\n",
        "    # Hidden layer\n",
        "    model.add(GRU(units = 200))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(units = 1))\n",
        "    #Compile model\n",
        "    model.compile(optimizer=\"adam\",\n",
        "                  loss='mean_absolute_error',\n",
        "                  metrics=['mean_absolute_error'])\n",
        "    # model_gru.summary()\n",
        "\n",
        "    # gru_history = model_gru.fit(X_train, y_train,\n",
        "    #                     epochs=1000,\n",
        "    #                     batch_size=256,\n",
        "    #                     shuffle=False,\n",
        "    #                     validation_split=0.2,\n",
        "    #                     verbose=2)\n",
        "\n",
        "    model.fit(X_train, y_train, epochs=1000, verbose=0)\n",
        "\n",
        "    # predictions\n",
        "    trainPredict = model.predict(X_train)\n",
        "    testPredict = model.predict(X_test)\n",
        "\n",
        "    trainPredict = scaler.inverse_transform(trainPredict)\n",
        "    testPredict  = scaler.inverse_transform(testPredict)\n",
        "    testY        = scaler.inverse_transform(y_test)\n",
        "\n",
        "    # metrics\n",
        "    mape= np.mean(np.abs(testPredict - testY)/np.abs(testY))\n",
        "    rmse= np.mean((testPredict-testY)**2)**.5\n",
        "    d= testPredict-testY\n",
        "    mae= np.mean(abs(d))\n",
        "\n",
        "    predicted.append(testPredict)\n",
        "\n",
        "    # print(\"MAPE: \", mape)\n",
        "    # print(\"RMSE: \", rmse)\n",
        "    # print(\"MAE: \", mae)\n",
        "\n",
        "    mapeList.append(mape)\n",
        "    rmseList.append(rmse)\n",
        "    maeList.append(mae)\n",
        "\n",
        "    i+=1\n",
        "\n",
        "# gru_fit=pd.DataFrame({'predicted': predicted})\n",
        "# gru_fit.to_csv('Trade_gru_preds.csv')\n",
        "\n",
        "gru_metrics4=pd.DataFrame({'MAPE' : mapeList,\n",
        "                    'RMSE': rmseList,\n",
        "                    'MAE': maeList})\n",
        "gru_metrics4.to_csv('Trade_gru_metrics4.csv')"
      ],
      "metadata": {
        "id": "Y12qr64cGAaA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca1d95d2-1ee9-4e59-d546-8152901193ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x780234b23130> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7802b4129b40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "LSTM\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SvP5J6fjlhkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2len=len(df2.columns)\n",
        "i=0\n",
        "\n",
        "testvals=[]\n",
        "predicted=[]\n",
        "\n",
        "mapeList=[]\n",
        "rmseList=[]\n",
        "maeList=[]\n",
        "\n",
        "while (i <= df2len-1):\n",
        "    dfi=df2.iloc[:,i]\n",
        "    dfi=dfi.reset_index(drop=True)\n",
        "    train=dfi[:24]\n",
        "    test=dfi[-7:]\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    sc_train = scaler.fit_transform(train.values.reshape(-1, 1))\n",
        "    train_scaled = scaler.transform(train.values.reshape(-1, 1))\n",
        "    sc_test = scaler.fit_transform(test.values.reshape(-1,1))\n",
        "    test_scaled = scaler.transform(test.values.reshape(-1,1))\n",
        "\n",
        "    LOOK_BACK = 1\n",
        "    X_train, y_train = create_dataset(train_scaled,LOOK_BACK)\n",
        "    X_test, y_test = create_dataset(test_scaled,LOOK_BACK)\n",
        "    # Print data shape\n",
        "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "    # print(\"X_train.shape:\", X_train.shape)\n",
        "    # print(\"y_train.shape:\", y_train.shape)\n",
        "    # print(\"X_test.shape:\", X_test.shape)\n",
        "    # print(\"y_test.shape:\", y_test.shape)\n",
        "\n",
        "\n",
        "    model_lstm = Sequential()\n",
        "    model_lstm.add(LSTM(200, input_shape=(LOOK_BACK,1),return_sequences = True))\n",
        "    model_lstm.add(LSTM(200, activation='relu',return_sequences = False))\n",
        "    model_lstm.add(Dense(units=1))\n",
        "\n",
        "    # adam = optimizers.Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=None, decay=1e-5, amsgrad=True)\n",
        "    model_lstm.compile(optimizer = 'adam',\n",
        "                  loss='mean_absolute_error',\n",
        "                  metrics=['mean_absolute_error'])\n",
        "    # model_lstm.summary()\n",
        "\n",
        "    # predictions\n",
        "    trainPredict = model_lstm.predict(X_train)\n",
        "    testPredict = model_lstm.predict(X_test)\n",
        "\n",
        "    trainPredict = scaler.inverse_transform(trainPredict)\n",
        "    testPredict  = scaler.inverse_transform(testPredict)\n",
        "    testY        = scaler.inverse_transform(y_test)\n",
        "\n",
        "    # metrics\n",
        "    mape= np.mean(np.abs(testPredict - testY)/np.abs(testY))\n",
        "    rmse= np.mean((testPredict-testY)**2)**.5\n",
        "    d= testPredict-testY\n",
        "    mae= np.mean(abs(d))\n",
        "\n",
        "    predicted.append(testPredict)\n",
        "\n",
        "    # print(\"MAPE: \", mape)\n",
        "    # print(\"RMSE: \", rmse)\n",
        "    # print(\"MAE: \", mae)\n",
        "\n",
        "    mapeList.append(mape)\n",
        "    rmseList.append(rmse)\n",
        "    maeList.append(mae)\n",
        "\n",
        "    i+=1\n",
        "#     print(testPredict)\n",
        "\n",
        "\n",
        "lstm_fit=pd.DataFrame({'predicted': predicted})\n",
        "lstm_fit.to_csv('Trade_lstm_preds.csv')\n",
        "\n",
        "lstm_metrics3=pd.DataFrame({'MAPE' : mapeList,\n",
        "                    'RMSE': rmseList,\n",
        "                    'MAE': maeList})\n",
        "lstm_metrics3.to_csv('Trade_lstm_metrics3.csv')"
      ],
      "metadata": {
        "id": "jJRHj-NGGVKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "CNN\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "seTnQrfxhio0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# univariate cnn example\n",
        "from numpy import array\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D"
      ],
      "metadata": {
        "id": "2gzpcttyRekA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2len=len(df2.columns)\n",
        "i=0\n",
        "\n",
        "testvals=[]\n",
        "predicted=[]\n",
        "\n",
        "mapeList=[]\n",
        "rmseList=[]\n",
        "maeList=[]\n",
        "\n",
        "while (i <= df2len-1):\n",
        "    dfi=df2.iloc[:,i]\n",
        "    dfi=dfi.reset_index(drop=True)\n",
        "    train=dfi[:24]\n",
        "    seq=train.values\n",
        "    test=dfi[-7:]\n",
        "    input=test.values\n",
        "\n",
        "    # choose a number of time steps\n",
        "    n_steps = 1\n",
        "    # split into samples\n",
        "    X_train, y_train = split_sequence(seq, n_steps)\n",
        "    X_test, y_test= split_sequence(input, n_steps)\n",
        "    # reshape from [samples, timesteps] into [samples, timesteps, features]\n",
        "    n_features = 1\n",
        "    # Print data shape\n",
        "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "    # define model\n",
        "    model_cnn = Sequential()\n",
        "    model_cnn.add(Conv1D(filters=200, kernel_size=1, activation='relu', input_shape=(n_steps,1)))\n",
        "    model_cnn.add(MaxPooling1D(pool_size=1))\n",
        "    model_cnn.add(Flatten())\n",
        "    model_cnn.add(Dense(50, activation='relu'))\n",
        "    model_cnn.add(Dense(1))\n",
        "    model_cnn.compile(optimizer = 'adam',\n",
        "                      loss='mean_absolute_error',\n",
        "                      metrics=['mean_absolute_error'])\n",
        "    # # fit model\n",
        "    model_cnn.fit(X_train, y_train, epochs=1000, verbose=0)\n",
        "    # Predictions\n",
        "    trainPredict = model_cnn.predict(X_train)\n",
        "    testPredict = model_cnn.predict(X_test)\n",
        "\n",
        "    mape= np.mean(np.abs(testPredict - y_test)/np.abs(y_test))\n",
        "    rmse= np.mean((testPredict-y_test)**2)**.5\n",
        "    d= testPredict-y_test\n",
        "    mae= np.mean(abs(d))\n",
        "\n",
        "    predicted.append(testPredict)\n",
        "\n",
        "    # print(\"MAPE: \", mape)\n",
        "    # print(\"RMSE: \", rmse)\n",
        "    # print(\"MAE: \", mae)\n",
        "\n",
        "    mapeList.append(mape)\n",
        "    rmseList.append(rmse)\n",
        "    maeList.append(mae)\n",
        "\n",
        "    i+=1\n",
        "\n",
        "cnn_fit=pd.DataFrame({'predicted': predicted})\n",
        "cnn_fit.to_csv('Trade_cnn_preds.csv')\n",
        "\n",
        "cnn_metrics2=pd.DataFrame({'MAPE' : mapeList,\n",
        "                    'RMSE': rmseList,\n",
        "                    'MAE': maeList})\n",
        "cnn_metrics2.to_csv('Trade_cnn_metrics2.csv')"
      ],
      "metadata": {
        "id": "FZiSWoEzOxJK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b05a38c-984e-4844-f3c8-692b62796444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7802361d7a30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x78023026f370> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ---\n",
        "\n",
        "CNN-GRU\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "5oWr-pV27LLL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmDF5zFxBhTF"
      },
      "source": [
        "# from keras.models import Model\n",
        "# from keras.layers import *\n",
        "# from keras.callbacks import EarlyStopping\n",
        "# from sklearn.model_selection import TimeSeriesSplit\n",
        "# import math\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# df2len=len(df2.columns)\n",
        "# i=0\n",
        "\n",
        "# testvals=[]\n",
        "# predicted=[]\n",
        "\n",
        "# mapeList=[]\n",
        "# rmseList=[]\n",
        "# maeList=[]\n",
        "\n",
        "# while (i <= df2len-1 ):\n",
        "#     dfi=df2.iloc[:,i]\n",
        "#     dfi=dfi.reset_index(drop=True)\n",
        "#     train=dfi[:-7] #24 train\n",
        "#     test=dfi[-7:] #7 test\n",
        "#     seq=train.values\n",
        "#     input=test.values\n",
        "\n",
        "#     scaler = MinMaxScaler()\n",
        "#     sc_train = scaler.fit_transform(train.values.reshape(-1, 1))\n",
        "#     train_scaled = scaler.transform(train.values.reshape(-1, 1))\n",
        "#     sc_test = scaler.fit_transform(test.values.reshape(-1,1))\n",
        "#     test_scaled = scaler.transform(test.values.reshape(-1,1))\n",
        "\n",
        "#     LOOK_BACK = 1\n",
        "#     X_train, y_train = create_dataset(train_scaled,LOOK_BACK)\n",
        "#     X_test, y_test = create_dataset(test_scaled,LOOK_BACK)\n",
        "#     # Print data shape\n",
        "#     X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "#     X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "#     n_steps = 1\n",
        "#     # # split into samples\n",
        "#     # X_train, y_train = split_sequence(seq, n_steps)\n",
        "#     # X_test, y_test= split_sequence(input, n_steps)\n",
        "#     # # reshape from [samples, timesteps] into [samples, timesteps, features]\n",
        "#     n_features = 1\n",
        "#     # X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "#     # X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "#     model = Sequential()\n",
        "#     model.add(Conv1D(64, 1, activation='relu'))\n",
        "#     model.add(SpatialDropout1D(0.2))\n",
        "#     model.add(MaxPooling1D(1))\n",
        "#     # tf.compat.v1.keras.layers.CuDNNGRU(64, activity_regularizer='tanh',\n",
        "#     # recurrent_regularizer= 'sigmoid',\n",
        "#     # bias_constraint=True,\n",
        "#     # return_sequences=False,\n",
        "#     # return_state=True)\n",
        "#     model.add(CuDNNGRU(64))\n",
        "#     model.add(Dropout(0.2))\n",
        "#     model.add(Dense(1, activation= 'sigmoid'))\n",
        "#     # model.summary()\n",
        "\n",
        "#     # model = Sequential()\n",
        "#     # model.add(Conv1D(filters=200, kernel_size=1, activation='relu', input_shape=(n_steps,1)))\n",
        "#     # model.add(MaxPooling1D(pool_size=1))\n",
        "#     # model.add(Flatten())\n",
        "#     # model.add(GRU(units = 200, return_sequences = True, activation='relu'))\n",
        "#     # model.add(Dropout(0.2))\n",
        "#     # model.add(GRU(units = 200))\n",
        "#     # model.add(Dropout(0.2))\n",
        "#     # model.add(Dense(units = 1))\n",
        "#     # model.add(Dense(50))\n",
        "#     # model.add(Dense(1))\n",
        "\n",
        "#     model.compile(optimizer = 'adam',\n",
        "#                       loss='mean_absolute_error',\n",
        "#                       metrics=['mean_absolute_error'])\n",
        "#     # # fit model\n",
        "#     model.fit(X_train, y_train, epochs=1000, verbose=0)\n",
        "#     # # Predictions\n",
        "#     trainPredict = model.predict(X_train)\n",
        "#     testPredict = model.predict(X_test)\n",
        "\n",
        "#     trainPredict = scaler.inverse_transform(trainPredict)\n",
        "#     testPredict  = scaler.inverse_transform(testPredict)\n",
        "#     testY        = scaler.inverse_transform(y_test)\n",
        "\n",
        "# # cng_fit=pd.DataFrame({'predicted': predicted})\n",
        "# # cng_fit.to_csv('rates_cng_preds.csv')\n",
        "\n",
        "# # cng_metrics=pd.DataFrame({'MAPE' : mapeList,\n",
        "# #                     'RMSE': rmseList,\n",
        "# #                     'MAE': maeList})\n",
        "# # cng_metrics.to_csv('rates_cng_metrics.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}