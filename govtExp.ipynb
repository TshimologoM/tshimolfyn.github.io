{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNu5KkUlgnRjcy+kHMgTsaC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TshimologoM/tshimolfyn.github.io/blob/main/govtExp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6JpZMrFKOrR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # !pip3 uninstall statsmodels\n",
        "# !pip3 install numpy scipy patsy pandas\n",
        "# !pip3 install statsmodels\n",
        "!pip install pmdarima\n",
        "# # !pip install --upgrade --no-deps statsmodels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z37Tas5c-B1o",
        "outputId": "3e4a151f-055d-48de-db7d-5185f601bfcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pmdarima\n",
            "  Downloading pmdarima-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.8 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.3.1)\n",
            "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (0.29.36)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.22.4)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.10.1)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (0.13.5)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.26.16)\n",
            "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pmdarima) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pmdarima) (2022.7.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->pmdarima) (3.1.0)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13.2->pmdarima) (0.5.3)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13.2->pmdarima) (23.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.2->statsmodels>=0.13.2->pmdarima) (1.16.0)\n",
            "Installing collected packages: pmdarima\n",
            "Successfully installed pmdarima-2.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PROJECT_ROOT_DIR = \".\"\n",
        "# CHAPTER_ID = \"end_to_end_project\"\n",
        "# IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "# os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "# def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "#     path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "#     print(\"Saving figure\", fig_id)\n",
        "#     if tight_layout:\n",
        "#         plt.tight_layout()\n",
        "#     plt.savefig(path, format=fig_extension, dpi=resolution)"
      ],
      "metadata": {
        "id": "K_Bi707qMAkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "id": "fTh5E1Q3Ag4T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "c7c94616-333c-49d9-8d3a-d937c0e105f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a3e8a890-0c16-452b-96b3-06cd8029f2e7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a3e8a890-0c16-452b-96b3-06cd8029f2e7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Govt Expenditure.csv to Govt Expenditure.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('Govt Expenditure.csv')\n",
        "\n",
        "# df.tail()"
      ],
      "metadata": {
        "id": "nANxHbJNMCeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfc=df.copy()\n",
        "# choose countries to use\n",
        "df1=dfc[((df[\"Country Name\"]== \"United States\")|(df[\"Country Name\"]== \"China\")|(df[\"Country Name\"]== \"Japan\")\n",
        "       |(df[\"Country Name\"]== \"Germany\")|(df[\"Country Name\"]== \"United Kingdom\")|(df[\"Country Name\"]== \"India\")\n",
        "       |(df[\"Country Name\"]== \"France\")|(df[\"Country Name\"]== \"Italy\")|(df[\"Country Name\"]== \"Canada\")|(df[\"Country Name\"]== \"Korea, Rep.\")\n",
        "       |(df[\"Country Name\"]== \"Russian Federation\")|(df[\"Country Name\"]== \"Brazil\")|(df[\"Country Name\"]== \"Australia\")|(df[\"Country Name\"]== \"Spain\")|(df[\"Country Name\"]== \"Mexico\"))]\n",
        "# drop unwanted columns\n",
        "df1=df1.drop(\"Country Code\",axis=1)\n",
        "df1=df1.drop(\"Indicator Code\",axis=1)\n",
        "df1=df1.drop(\"Indicator Name\",axis=1)\n",
        "df1=df1.drop(df1.loc[:, '1960':'1989'].columns, axis = 1)\n",
        "df1=df1.drop(\"2021\",axis=1)\n",
        "# reset and rename\n",
        "df1.reset_index(drop=True,inplace=True)\n",
        "df1.rename(columns = {'Country Name':'Country'}, inplace = True)\n",
        "df1['Country'] = df1['Country'].replace(['United Kingdom'],'UK')\n",
        "df1['Country'] = df1['Country'].replace(['United States'],'USA')\n",
        "df1['Country'] = df1['Country'].replace(['Korea, Rep.'],'Korea')\n",
        "df1['Country'] = df1['Country'].replace(['Russian Federation'],'Russia')\n",
        "# df1.head(15)"
      ],
      "metadata": {
        "id": "CgZA5G8LA09a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "melted_df = pd.melt(df1, id_vars=[\"Country\"], var_name=\"Year\", value_name=\"GDP\")\n",
        "gdp_melted_df=melted_df.copy()\n",
        "gdp_melted_df['Year']=pd.to_datetime(gdp_melted_df['Year'])\n",
        "df3=gdp_melted_df[['Year','GDP']].set_index('Year')\n",
        "gdp_melted_df['Year']=pd.to_datetime(gdp_melted_df['Year'])\n",
        "df2 = melted_df.pivot(index='Year', columns='Country', values='GDP')\n",
        "# display(melted_df.head(15))\n",
        "# display(gdp_melted_df.head(15))\n",
        "# display(df2.tail(15))\n",
        "# display(df3.tail(15))"
      ],
      "metadata": {
        "id": "aDTfGBQlBCFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df2.reset_index()\n",
        "df2['Year'] = pd.to_datetime(df2['Year'])\n",
        "df2 = df2.set_index('Year')\n",
        "# df2.tail(6)"
      ],
      "metadata": {
        "id": "mzizVijXDjb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total num of missing values:\")\n",
        "print(df2.isna().sum())\n",
        "print(\"\")\n",
        "# df2.info()"
      ],
      "metadata": {
        "id": "GqswOWnxIay9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ff27f84-f51c-4792-f0a4-46e4c699b0ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total num of missing values:\n",
            "Country\n",
            "Australia    0\n",
            "Brazil       0\n",
            "Canada       0\n",
            "China        0\n",
            "France       0\n",
            "Germany      0\n",
            "India        0\n",
            "Italy        0\n",
            "Japan        0\n",
            "Korea        0\n",
            "Mexico       0\n",
            "Russia       0\n",
            "Spain        0\n",
            "UK           0\n",
            "USA          0\n",
            "dtype: int64\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.style.use('fivethirtyeight')\n",
        "# plt.rcParams.update({'figure.figsize':(24,12), 'figure.dpi':100})\n",
        "\n",
        "# fig, ((ax1, ax2, ax3, ax4), (ax5, ax6, ax7, ax8),\n",
        "#       (ax9, ax10, ax11, ax12), (ax13, ax14, ax15, ax16)) = plt.subplots(4, 4)\n",
        "# # fig.suptitle(\"Country's government expenditure trend plot\")\n",
        "\n",
        "# ax1.plot(df2[\"Australia\"])\n",
        "# ax1.set_title('Australia')\n",
        "# ax2.plot(df2[\"Brazil\"])\n",
        "# ax2.set_title('Brazil')\n",
        "# ax3.plot(df2[\"Canada\"])\n",
        "# ax3.set_title('Canada')\n",
        "# ax4.plot(df2[\"China\"])\n",
        "# ax4.set_title('China')\n",
        "# ax5.plot(df2[\"France\"])\n",
        "# ax5.set_title('France')\n",
        "# ax6.plot(df2[\"Germany\"])\n",
        "# ax6.set_title('Germany')\n",
        "# ax7.plot(df2[\"India\"])\n",
        "# ax7.set_title('India')\n",
        "# ax8.plot(df2[\"Italy\"])\n",
        "# ax8.set_title('Italy')\n",
        "# ax9.plot(df2[\"Japan\"])\n",
        "# ax9.set_title('Japan')\n",
        "# ax10.plot(df2[\"Korea\"])\n",
        "# ax10.set_title('Korea')\n",
        "# ax11.plot(df2[\"Mexico\"])\n",
        "# ax11.set_title('Mexico')\n",
        "# ax12.plot(df2[\"Russia\"])\n",
        "# ax12.set_title('Russia')\n",
        "# ax13.plot(df2[\"Spain\"])\n",
        "# ax13.set_title('Spain')\n",
        "# ax14.plot(df2[\"UK\"])\n",
        "# ax14.set_title('UK')\n",
        "# ax15.plot(df2[\"USA\"])\n",
        "# ax15.set_title('USA')\n",
        "# fig.delaxes(ax16)\n",
        "\n",
        "# plt.tight_layout()\n",
        "# fig.subplots_adjust(top=1.2)\n",
        "# for ax in fig.get_axes():\n",
        "#     ax.set(xlabel='Year', ylabel='Values')\n",
        "\n",
        "# # Hide x labels and tick labels for top plots and y ticks for right plots.\n",
        "# # for ax in fig.get_axes():\n",
        "# #     ax.label_outer()\n",
        "\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "bG6Z_IctMGl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.rcParams.update({'figure.figsize':(19,18), 'figure.dpi':100})\n",
        "\n",
        "# fig, ((ax1, ax2, ax3), (ax4, ax5, ax6), (ax7, ax8, ax9),\n",
        "#       (ax10, ax11, ax12), (ax13, ax14, ax15)) = plt.subplots(5, 3)\n",
        "# # fig.suptitle(\"Country's government expenditure trend plot\")\n",
        "\n",
        "# ax1.plot(df2[\"Australia\"])\n",
        "# ax1.set_title('Australia')\n",
        "# ax2.plot(df2[\"Brazil\"])\n",
        "# ax2.set_title('Brazil')\n",
        "# ax3.plot(df2[\"Canada\"])\n",
        "# ax3.set_title('Canada')\n",
        "# ax4.plot(df2[\"China\"])\n",
        "# ax4.set_title('China')\n",
        "# ax5.plot(df2[\"France\"])\n",
        "# ax5.set_title('France')\n",
        "# ax6.plot(df2[\"Germany\"])\n",
        "# ax6.set_title('Germany')\n",
        "# ax7.plot(df2[\"India\"])\n",
        "# ax7.set_title('India')\n",
        "# ax8.plot(df2[\"Italy\"])\n",
        "# ax8.set_title('Italy')\n",
        "# ax9.plot(df2[\"Japan\"])\n",
        "# ax9.set_title('Japan')\n",
        "# ax10.plot(df2[\"Korea\"])\n",
        "# ax10.set_title('Korea')\n",
        "# ax11.plot(df2[\"Mexico\"])\n",
        "# ax11.set_title('Mexico')\n",
        "# ax12.plot(df2[\"Russia\"])\n",
        "# ax12.set_title('Russia')\n",
        "# ax13.plot(df2[\"Spain\"])\n",
        "# ax13.set_title('Spain')\n",
        "# ax14.plot(df2[\"UK\"])\n",
        "# ax14.set_title('UK')\n",
        "# ax15.plot(df2[\"USA\"])\n",
        "# ax15.set_title('USA')\n",
        "\n",
        "# plt.tight_layout()\n",
        "# fig.subplots_adjust(right=1.3, top=1.2)\n",
        "# # fig.set_facecolor('white')\n",
        "# for ax in fig.get_axes():\n",
        "#     ax.set(xlabel='Year', ylabel='Values')\n",
        "#     # ax.set_facecolor('white')\n",
        "\n",
        "# # Hide x labels and tick labels for top plots and y ticks for right plots.\n",
        "# # for ax in fig.get_axes():\n",
        "#     # ax.label_outer()\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "1RgD4h7t3Xyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df2.describe().T"
      ],
      "metadata": {
        "id": "XX4RsNWt1bNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pandas profiling TRY\n",
        "# import pandas_profiling as pp\n",
        "# eda = pp.ProfileReport(df)\n",
        "# display(eda)"
      ],
      "metadata": {
        "id": "r2RmKb7udvWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SVR\n",
        "import matplotlib.dates as mdates\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVR\n",
        "import math\n",
        "from math import sqrt\n",
        "import sklearn.metrics\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error,mean_absolute_percentage_error, r2_score\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "bs8uNsdT8tgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.head()\n",
        "dfi=df2.iloc[:,7]\n",
        "dfi.head()\n",
        "dfis=dfi.values\n",
        "dfis\n",
        "df2len=len(df2.columns)\n",
        "df2len"
      ],
      "metadata": {
        "id": "YkqEHppRr9wA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa907c11-88f9-4b85-8589-85d4fc2dbea6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2len=len(df2.columns)\n",
        "i=0\n",
        "\n",
        "testvals=[]\n",
        "predicted=[]\n",
        "\n",
        "mapeList=[]\n",
        "rmseList=[]\n",
        "maeList=[]\n",
        "\n",
        "while (i <= df2len-1):\n",
        "    dfi=df2.iloc[:,i]\n",
        "    train=dfi[:25]\n",
        "    test=dfi[-6:]\n",
        "    # display(train.head())\n",
        "    # display(train.info())\n",
        "    # display(test.head())\n",
        "    # print(test.info())\n",
        "\n",
        "    # change the dates into ints for training\n",
        "    dates_df = train.copy() #!!!!!!!!!!!!!!!!\n",
        "    dates_df = dates_df.reset_index()\n",
        "\n",
        "    # Store the original dates for plotting the predicitons\n",
        "    org_dates = dates_df['Year']\n",
        "\n",
        "    # convert to ints\n",
        "    dates_df['Year'] = dates_df['Year'].map(mdates.date2num)\n",
        "\n",
        "    dates_df.head()\n",
        "    ####\n",
        "    # change the dates into ints for testing\n",
        "    dates_test_df = test.copy() #!!!!!!!!!!!!!!!!\n",
        "    dates_test_df = dates_test_df.reset_index()\n",
        "\n",
        "    # Store the original dates for plotting the predicitons\n",
        "    org_test_dates = dates_test_df['Year']\n",
        "\n",
        "    # convert to ints\n",
        "    dates_test_df['Year'] = dates_test_df['Year'].map(mdates.date2num)\n",
        "\n",
        "    # print(dates_test_df.head())\n",
        "    # creating test and train for svr fitting\n",
        "    X_train=dates_df['Year']\n",
        "    # d=dates_df.iloc[:,1]\n",
        "    # print(d)\n",
        "    y_train=dates_df.iloc[:,1]\n",
        "    X_test=dates_test_df['Year']\n",
        "    y_test=dates_test_df.iloc[:,1]\n",
        "\n",
        "    # Feature Scaling\n",
        "    sc_X = StandardScaler()\n",
        "    sc_y = StandardScaler()\n",
        "    X_train = sc_X.fit_transform(X_train.values.reshape(-1, 1))\n",
        "    y_train = sc_y.fit_transform(y_train.values.reshape(-1, 1))\n",
        "\n",
        "    # Fitting SVR to the dataset\n",
        "    regressor = SVR(kernel = 'rbf')\n",
        "    regressor.fit(X_train, y_train)\n",
        "    # Predicting a new result\n",
        "    y_pred= regressor.predict(sc_X.transform(X_test.values.reshape(-1, 1)))\n",
        "    dim2y_pred = sc_y.inverse_transform(y_pred.reshape(-1, 1))\n",
        "\n",
        "    testvals.append(y_test)\n",
        "    predicted.append(dim2y_pred)\n",
        "    # print(\"test values\")\n",
        "    # display(y_test)\n",
        "    # print(\"predicted values\")\n",
        "    # display(y_pred)\n",
        "\n",
        "    mape= np.mean(np.abs(y_pred - y_test)/np.abs(y_test))\n",
        "    rmse= np.mean((y_pred-y_test)**2)**.5\n",
        "    d= y_pred-y_test.values\n",
        "    mae= np.mean(abs(d))\n",
        "\n",
        "    # print(\"MAPE: \", mape)\n",
        "    # print(\"RMSE: \", rmse)\n",
        "    # print(\"MAE: \", mae)\n",
        "\n",
        "    mapeList.append(mape)\n",
        "    rmseList.append(rmse)\n",
        "    maeList.append(mae)\n",
        "\n",
        "    i+=1\n",
        "\n",
        "svr_fit=pd.DataFrame({'exact' : testvals,\n",
        "                    'predicted': predicted})\n",
        "svr_fit.to_csv('exp_svr_preds.csv')\n",
        "\n",
        "svr_metrics=pd.DataFrame({'MAPE' : mapeList,\n",
        "                    'RMSE': rmseList,\n",
        "                    'MAE': maeList})\n",
        "svr_metrics.to_csv('exp_svr_metrics.csv')"
      ],
      "metadata": {
        "id": "RqglczrcQzhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "ARIMA\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZLS3EKksqFsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "import pmdarima as pm\n",
        "\n",
        "df2len=len(df2.columns)\n",
        "i=0\n",
        "\n",
        "predicted=[]\n",
        "\n",
        "mapeList=[]\n",
        "rmseList=[]\n",
        "maeList=[]\n",
        "\n",
        "while (i <= df2len-1):\n",
        "    series=df2.iloc[:,i]\n",
        "\n",
        "    model = pm.auto_arima(series, start_p=1, start_q=1,\n",
        "                          test='adf',       # use adftest to find optimal 'd'\n",
        "                          max_p=3, max_q=3, # maximum p and q\n",
        "                          m=1,              # frequency of series\n",
        "                          d=None,           # let model determine 'd'\n",
        "                          seasonal=False,   # No Seasonality\n",
        "                          start_P=0,\n",
        "                          D=0,\n",
        "                          trace=True,\n",
        "                          error_action='ignore',\n",
        "                          suppress_warnings=True,\n",
        "                          stepwise=True)\n",
        "\n",
        "    #     print(model.summary())\n",
        "    values = series.values\n",
        "    n_periods = len(series)\n",
        "    fc, confint = model.predict(n_periods=n_periods, return_conf_int=True)\n",
        "\n",
        "    predicted.append(fc)\n",
        "    # print(\"predicted values\")\n",
        "    # display(fc)\n",
        "\n",
        "    mape= np.mean(np.abs(fc - values)/np.abs(values))\n",
        "    rmse= np.mean((fc-values)**2)**.5\n",
        "    d= fc-values\n",
        "    mae= np.mean(abs(d))\n",
        "\n",
        "    # print(\"MAPE: \", mape)\n",
        "    # print(\"RMSE: \", rmse)\n",
        "    # print(\"MAE: \", mae)\n",
        "\n",
        "    mapeList.append(mape)\n",
        "    rmseList.append(rmse)\n",
        "    maeList.append(mae)\n",
        "\n",
        "    i+=1\n",
        "\n",
        "asmr_fit=pd.DataFrame({'predicted': predicted})\n",
        "asmr_fit.to_csv('exp_arima_preds.csv')\n",
        "\n",
        "asmr_metrics=pd.DataFrame({'MAPE' : mapeList,\n",
        "                    'RMSE': rmseList,\n",
        "                    'MAE': maeList})\n",
        "asmr_metrics.to_csv('exp_arima_metrics.csv')"
      ],
      "metadata": {
        "id": "mrW7kTLQZ3iG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cff51816-44b0-49fa-b7d5-a7920e2e3b7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing stepwise search to minimize aic\n",
            " ARIMA(1,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.30 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0] intercept   : AIC=35.950, Time=0.02 sec\n",
            " ARIMA(1,2,0)(0,0,0)[0] intercept   : AIC=37.835, Time=0.03 sec\n",
            " ARIMA(0,2,1)(0,0,0)[0] intercept   : AIC=37.721, Time=0.09 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0]             : AIC=33.985, Time=0.02 sec\n",
            "\n",
            "Best model:  ARIMA(0,2,0)(0,0,0)[0]          \n",
            "Total fit time: 0.489 seconds\n",
            "Performing stepwise search to minimize aic\n",
            " ARIMA(1,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.20 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0] intercept   : AIC=96.225, Time=0.01 sec\n",
            " ARIMA(1,2,0)(0,0,0)[0] intercept   : AIC=90.953, Time=0.03 sec\n",
            " ARIMA(0,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.07 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0]             : AIC=94.305, Time=0.02 sec\n",
            " ARIMA(2,2,0)(0,0,0)[0] intercept   : AIC=88.907, Time=0.03 sec\n",
            " ARIMA(3,2,0)(0,0,0)[0] intercept   : AIC=89.114, Time=0.04 sec\n",
            " ARIMA(2,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.16 sec\n",
            " ARIMA(3,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.22 sec\n",
            " ARIMA(2,2,0)(0,0,0)[0]             : AIC=87.086, Time=0.03 sec\n",
            " ARIMA(1,2,0)(0,0,0)[0]             : AIC=89.121, Time=0.02 sec\n",
            " ARIMA(3,2,0)(0,0,0)[0]             : AIC=87.283, Time=0.03 sec\n",
            " ARIMA(2,2,1)(0,0,0)[0]             : AIC=inf, Time=0.17 sec\n",
            " ARIMA(1,2,1)(0,0,0)[0]             : AIC=inf, Time=0.12 sec\n",
            " ARIMA(3,2,1)(0,0,0)[0]             : AIC=inf, Time=0.13 sec\n",
            "\n",
            "Best model:  ARIMA(2,2,0)(0,0,0)[0]          \n",
            "Total fit time: 1.316 seconds\n",
            "Performing stepwise search to minimize aic\n",
            " ARIMA(1,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.18 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0] intercept   : AIC=75.872, Time=0.03 sec\n",
            " ARIMA(1,2,0)(0,0,0)[0] intercept   : AIC=76.659, Time=0.03 sec\n",
            " ARIMA(0,2,1)(0,0,0)[0] intercept   : AIC=75.972, Time=0.04 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0]             : AIC=73.880, Time=0.02 sec\n",
            "\n",
            "Best model:  ARIMA(0,2,0)(0,0,0)[0]          \n",
            "Total fit time: 0.301 seconds\n",
            "Performing stepwise search to minimize aic\n",
            " ARIMA(1,2,1)(0,0,0)[0] intercept   : AIC=45.701, Time=0.16 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0] intercept   : AIC=42.172, Time=0.03 sec\n",
            " ARIMA(1,2,0)(0,0,0)[0] intercept   : AIC=43.962, Time=0.04 sec\n",
            " ARIMA(0,2,1)(0,0,0)[0] intercept   : AIC=43.868, Time=0.04 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0]             : AIC=40.197, Time=0.02 sec\n",
            "\n",
            "Best model:  ARIMA(0,2,0)(0,0,0)[0]          \n",
            "Total fit time: 0.288 seconds\n",
            "Performing stepwise search to minimize aic\n",
            " ARIMA(1,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.23 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0] intercept   : AIC=68.177, Time=0.02 sec\n",
            " ARIMA(1,2,0)(0,0,0)[0] intercept   : AIC=68.601, Time=0.03 sec\n",
            " ARIMA(0,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.10 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0]             : AIC=66.341, Time=0.02 sec\n",
            "\n",
            "Best model:  ARIMA(0,2,0)(0,0,0)[0]          \n",
            "Total fit time: 0.425 seconds\n",
            "Performing stepwise search to minimize aic\n",
            " ARIMA(1,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.23 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0] intercept   : AIC=64.484, Time=0.03 sec\n",
            " ARIMA(1,2,0)(0,0,0)[0] intercept   : AIC=65.562, Time=0.03 sec\n",
            " ARIMA(0,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.10 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0]             : AIC=62.947, Time=0.01 sec\n",
            "\n",
            "Best model:  ARIMA(0,2,0)(0,0,0)[0]          \n",
            "Total fit time: 0.425 seconds\n",
            "Performing stepwise search to minimize aic\n",
            " ARIMA(1,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.19 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0] intercept   : AIC=48.630, Time=0.02 sec\n",
            " ARIMA(1,2,0)(0,0,0)[0] intercept   : AIC=47.776, Time=0.03 sec\n",
            " ARIMA(0,2,1)(0,0,0)[0] intercept   : AIC=46.860, Time=0.03 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0]             : AIC=46.904, Time=0.02 sec\n",
            " ARIMA(0,2,2)(0,0,0)[0] intercept   : AIC=inf, Time=0.12 sec\n",
            " ARIMA(1,2,2)(0,0,0)[0] intercept   : AIC=inf, Time=0.18 sec\n",
            " ARIMA(0,2,1)(0,0,0)[0]             : AIC=45.360, Time=0.03 sec\n",
            " ARIMA(1,2,1)(0,0,0)[0]             : AIC=44.982, Time=0.07 sec\n",
            " ARIMA(1,2,0)(0,0,0)[0]             : AIC=46.167, Time=0.02 sec\n",
            " ARIMA(2,2,1)(0,0,0)[0]             : AIC=46.947, Time=0.11 sec\n",
            " ARIMA(1,2,2)(0,0,0)[0]             : AIC=46.956, Time=0.09 sec\n",
            " ARIMA(0,2,2)(0,0,0)[0]             : AIC=45.681, Time=0.06 sec\n",
            " ARIMA(2,2,0)(0,0,0)[0]             : AIC=47.659, Time=0.03 sec\n",
            " ARIMA(2,2,2)(0,0,0)[0]             : AIC=inf, Time=0.22 sec\n",
            "\n",
            "Best model:  ARIMA(1,2,1)(0,0,0)[0]          \n",
            "Total fit time: 1.252 seconds\n",
            "Performing stepwise search to minimize aic\n",
            " ARIMA(1,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.16 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0] intercept   : AIC=65.554, Time=0.02 sec\n",
            " ARIMA(1,2,0)(0,0,0)[0] intercept   : AIC=66.479, Time=0.03 sec\n",
            " ARIMA(0,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.09 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0]             : AIC=63.880, Time=0.02 sec\n",
            "\n",
            "Best model:  ARIMA(0,2,0)(0,0,0)[0]          \n",
            "Total fit time: 0.342 seconds\n",
            "Performing stepwise search to minimize aic\n",
            " ARIMA(1,2,1)(0,0,0)[0] intercept   : AIC=40.352, Time=0.09 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0] intercept   : AIC=44.178, Time=0.03 sec\n",
            " ARIMA(1,2,0)(0,0,0)[0] intercept   : AIC=38.359, Time=0.03 sec\n",
            " ARIMA(0,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.15 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0]             : AIC=42.392, Time=0.02 sec\n",
            " ARIMA(2,2,0)(0,0,0)[0] intercept   : AIC=40.351, Time=0.04 sec\n",
            " ARIMA(2,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.26 sec\n",
            " ARIMA(1,2,0)(0,0,0)[0]             : AIC=36.612, Time=0.03 sec\n",
            " ARIMA(2,2,0)(0,0,0)[0]             : AIC=38.599, Time=0.03 sec\n",
            " ARIMA(1,2,1)(0,0,0)[0]             : AIC=38.601, Time=0.04 sec\n",
            " ARIMA(0,2,1)(0,0,0)[0]             : AIC=inf, Time=0.28 sec\n",
            " ARIMA(2,2,1)(0,0,0)[0]             : AIC=inf, Time=0.17 sec\n",
            "\n",
            "Best model:  ARIMA(1,2,0)(0,0,0)[0]          \n",
            "Total fit time: 1.183 seconds\n",
            "Performing stepwise search to minimize aic\n",
            " ARIMA(1,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.19 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0] intercept   : AIC=56.148, Time=0.02 sec\n",
            " ARIMA(1,2,0)(0,0,0)[0] intercept   : AIC=48.459, Time=0.03 sec\n",
            " ARIMA(0,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.11 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0]             : AIC=54.289, Time=0.01 sec\n",
            " ARIMA(2,2,0)(0,0,0)[0] intercept   : AIC=44.111, Time=0.05 sec\n",
            " ARIMA(3,2,0)(0,0,0)[0] intercept   : AIC=46.030, Time=0.05 sec\n",
            " ARIMA(2,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.18 sec\n",
            " ARIMA(3,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.22 sec\n",
            " ARIMA(2,2,0)(0,0,0)[0]             : AIC=42.950, Time=0.03 sec\n",
            " ARIMA(1,2,0)(0,0,0)[0]             : AIC=46.810, Time=0.02 sec\n",
            " ARIMA(3,2,0)(0,0,0)[0]             : AIC=44.931, Time=0.03 sec\n",
            " ARIMA(2,2,1)(0,0,0)[0]             : AIC=44.557, Time=0.06 sec\n",
            " ARIMA(1,2,1)(0,0,0)[0]             : AIC=42.772, Time=0.04 sec\n",
            " ARIMA(0,2,1)(0,0,0)[0]             : AIC=40.947, Time=0.11 sec\n",
            " ARIMA(0,2,2)(0,0,0)[0]             : AIC=42.749, Time=0.06 sec\n",
            " ARIMA(1,2,2)(0,0,0)[0]             : AIC=inf, Time=0.35 sec\n",
            "\n",
            "Best model:  ARIMA(0,2,1)(0,0,0)[0]          \n",
            "Total fit time: 1.597 seconds\n",
            "Performing stepwise search to minimize aic\n",
            " ARIMA(1,0,1)(0,0,0)[0]             : AIC=61.646, Time=0.16 sec\n",
            " ARIMA(0,0,0)(0,0,0)[0]             : AIC=236.303, Time=0.02 sec\n",
            " ARIMA(1,0,0)(0,0,0)[0]             : AIC=inf, Time=0.08 sec\n",
            " ARIMA(0,0,1)(0,0,0)[0]             : AIC=inf, Time=0.11 sec\n",
            " ARIMA(2,0,1)(0,0,0)[0]             : AIC=inf, Time=0.30 sec\n",
            " ARIMA(1,0,2)(0,0,0)[0]             : AIC=62.744, Time=0.18 sec\n",
            " ARIMA(0,0,2)(0,0,0)[0]             : AIC=inf, Time=0.19 sec\n",
            " ARIMA(2,0,0)(0,0,0)[0]             : AIC=inf, Time=0.14 sec\n",
            " ARIMA(2,0,2)(0,0,0)[0]             : AIC=inf, Time=0.38 sec\n",
            " ARIMA(1,0,1)(0,0,0)[0] intercept   : AIC=59.545, Time=0.20 sec\n",
            " ARIMA(0,0,1)(0,0,0)[0] intercept   : AIC=87.770, Time=0.06 sec\n",
            " ARIMA(1,0,0)(0,0,0)[0] intercept   : AIC=57.651, Time=0.12 sec\n",
            " ARIMA(0,0,0)(0,0,0)[0] intercept   : AIC=111.741, Time=0.01 sec\n",
            " ARIMA(2,0,0)(0,0,0)[0] intercept   : AIC=59.517, Time=0.08 sec\n",
            " ARIMA(2,0,1)(0,0,0)[0] intercept   : AIC=61.651, Time=0.11 sec\n",
            "\n",
            "Best model:  ARIMA(1,0,0)(0,0,0)[0] intercept\n",
            "Total fit time: 2.201 seconds\n",
            "Performing stepwise search to minimize aic\n",
            " ARIMA(1,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.15 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0] intercept   : AIC=145.137, Time=0.01 sec\n",
            " ARIMA(1,2,0)(0,0,0)[0] intercept   : AIC=144.105, Time=0.03 sec\n",
            " ARIMA(0,2,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.11 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0]             : AIC=143.334, Time=0.01 sec\n",
            "\n",
            "Best model:  ARIMA(0,2,0)(0,0,0)[0]          \n",
            "Total fit time: 0.343 seconds\n",
            "Performing stepwise search to minimize aic\n",
            " ARIMA(1,2,1)(0,0,0)[0] intercept   : AIC=72.321, Time=0.10 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0] intercept   : AIC=68.806, Time=0.03 sec\n",
            " ARIMA(1,2,0)(0,0,0)[0] intercept   : AIC=70.547, Time=0.03 sec\n",
            " ARIMA(0,2,1)(0,0,0)[0] intercept   : AIC=70.579, Time=0.03 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0]             : AIC=67.155, Time=0.02 sec\n",
            "\n",
            "Best model:  ARIMA(0,2,0)(0,0,0)[0]          \n",
            "Total fit time: 0.237 seconds\n",
            "Performing stepwise search to minimize aic\n",
            " ARIMA(1,2,1)(0,0,0)[0] intercept   : AIC=82.767, Time=0.06 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0] intercept   : AIC=79.007, Time=0.02 sec\n",
            " ARIMA(1,2,0)(0,0,0)[0] intercept   : AIC=80.773, Time=0.04 sec\n",
            " ARIMA(0,2,1)(0,0,0)[0] intercept   : AIC=80.786, Time=0.04 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0]             : AIC=77.326, Time=0.02 sec\n",
            "\n",
            "Best model:  ARIMA(0,2,0)(0,0,0)[0]          \n",
            "Total fit time: 0.198 seconds\n",
            "Performing stepwise search to minimize aic\n",
            " ARIMA(1,2,1)(0,0,0)[0] intercept   : AIC=26.686, Time=0.07 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0] intercept   : AIC=25.010, Time=0.02 sec\n",
            " ARIMA(1,2,0)(0,0,0)[0] intercept   : AIC=25.093, Time=0.03 sec\n",
            " ARIMA(0,2,1)(0,0,0)[0] intercept   : AIC=24.767, Time=0.03 sec\n",
            " ARIMA(0,2,0)(0,0,0)[0]             : AIC=23.056, Time=0.02 sec\n",
            "\n",
            "Best model:  ARIMA(0,2,0)(0,0,0)[0]          \n",
            "Total fit time: 0.175 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "ETS\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GJX7KNz-qSnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ETS\n",
        "from statsmodels.tsa.api import ExponentialSmoothing\n",
        "\n",
        "df2len=len(df2.columns)\n",
        "i=0\n",
        "\n",
        "predictedets=[]\n",
        "\n",
        "emapeList=[]\n",
        "ermseList=[]\n",
        "emaeList=[]\n",
        "\n",
        "while (i <= df2len-1):\n",
        "    dfi=df2.iloc[:,i]\n",
        "    train=dfi[:25]\n",
        "    test=dfi[-6:]\n",
        "\n",
        "    xlabel_name = 'Year'\n",
        "    ylabel_name = 'GDP'\n",
        "    # pre_name = 'GDP'\n",
        "    train_ETS = train.values#!!!!!!!!\n",
        "    test_ETS = test.values#!!!!!!!!\n",
        "\n",
        "    history = [x for x in train_ETS]\n",
        "    y_ets = test_ETS\n",
        "    # make first prediction\n",
        "    predictions=list()\n",
        "    for t in range(len(test)):#!!!!!!!!!!!!!!\n",
        "        model_ets = ExponentialSmoothing(history)\n",
        "        # model_fit = model.fit()\n",
        "        # model_fit = model.fit(smoothing_level=0.9,optimized=False)\n",
        "        model_fit = model_ets.fit(smoothing_level=0.44,smoothing_slope=0.011)\n",
        "        yhat = model_fit.forecast()[0]\n",
        "        predictions.append(yhat)\n",
        "        obs = y_ets[t]\n",
        "        history.append(obs)\n",
        "\n",
        "    emape= np.mean(np.abs(yhat - y_ets)/np.abs(y_ets))\n",
        "    ermse= np.mean((yhat-y_ets)**2)**.5\n",
        "    ed= yhat-y_ets\n",
        "    emae= np.mean(abs(ed))\n",
        "\n",
        "    emapeList.append(emape)\n",
        "    ermseList.append(ermse)\n",
        "    emaeList.append(emae)\n",
        "\n",
        "        # display(predictions)\n",
        "\n",
        "    predictedets.append(predictions)\n",
        "\n",
        "\n",
        "\n",
        "    # print(\"MAPE: \", mape)\n",
        "    # print(\"RMSE: \", rmse)\n",
        "    # print(\"MAE: \", mae)\n",
        "\n",
        "    i+=1\n",
        "\n",
        "ets_fit=pd.DataFrame({'predicted': predictedets})\n",
        "ets_fit.to_csv('exp_ets_preds.csv')\n",
        "\n",
        "ets_metrics=pd.DataFrame({'MAPE' : emapeList,\n",
        "                    'RMSE': ermseList,\n",
        "                    'MAE': emaeList})\n",
        "ets_metrics.to_csv('exp_ets_metrics.csv')"
      ],
      "metadata": {
        "id": "4FawPofonz3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "XGBOOST\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "OVkrggmix87Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost\n",
        "print(\"xgboost\", xgboost.__version__)"
      ],
      "metadata": {
        "id": "_Uk8rNilux6v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f02344d3-9051-46eb-de1b-0b68379a9ed3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xgboost 1.7.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import asarray\n",
        "from pandas import concat\n",
        "from pandas import DataFrame\n",
        "from xgboost import XGBRegressor\n"
      ],
      "metadata": {
        "id": "6De-eSGdypou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transform a time series dataset into a supervised learning dataset\n",
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "\tn_vars = 1 if type(data) is list else data.shape[0]\n",
        "\tdf = DataFrame(data)\n",
        "\tcols = list()\n",
        "\t# input sequence (t-n, ... t-1)\n",
        "\tfor i in range(n_in, 0, -1):\n",
        "\t\tcols.append(df.shift(i))\n",
        "\t# forecast sequence (t, t+1, ... t+n)\n",
        "\tfor i in range(0, n_out):\n",
        "\t\tcols.append(df.shift(-i))\n",
        "\t# put it all together\n",
        "\tagg = concat(cols, axis=1)\n",
        "\t# drop rows with NaN values\n",
        "\tif dropnan:\n",
        "\t\tagg.dropna(inplace=True)\n",
        "\treturn agg.values\n",
        "#\n",
        "# split a univariate dataset into train/test sets\n",
        "def train_test_split(data, n_test):\n",
        "\treturn data[:-n_test, :], data[-n_test:, :]\n",
        "\n",
        "# fit an xgboost model and make a one step prediction\n",
        "def xgboost_forecast(train, testX):\n",
        "\t# transform list into array\n",
        "\ttrain = asarray(train)\n",
        "\t# split into input and output columns\n",
        "\ttrainX, trainy = train[:, :-1], train[:, -1]\n",
        "\t# fit model\n",
        "\tmodel = XGBRegressor(objective='reg:squarederror', n_estimators=1000)\n",
        "\tmodel.fit(trainX, trainy)\n",
        "\t# make a one-step prediction\n",
        "\tyhat = model.predict(asarray([testX]))\n",
        "\treturn yhat[0]"
      ],
      "metadata": {
        "id": "Cw-judQHyx8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dataset\n",
        "\n",
        "dataset_col =df2\n",
        "df2len=len(df2.columns)\n",
        "i=0\n",
        "\n",
        "predictedxg=[]\n",
        "\n",
        "xmapeList=[]\n",
        "xrmseList=[]\n",
        "xmaeList=[]\n",
        "\n",
        "while (i <= df2len-1):\n",
        "    series=df2.iloc[:,i]\n",
        "    values = series.values\n",
        "    # transform the time series data into supervised learning\n",
        "    data = series_to_supervised(values, n_in=6)\n",
        "\n",
        "# walk-forward validation for univariate data\n",
        "\n",
        "    predictions = list()\n",
        "    # split dataset\n",
        "    train, test = train_test_split(data, 6)\n",
        "    # seed history with training dataset\n",
        "    history_xg = [x for x in train]\n",
        "    # step over each time-step in the test set\n",
        "    for j in range(len(test)):\n",
        "        # split test row into input and output columns\n",
        "        testX, testy = test[j, :-1], test[j, -1]\n",
        "        # fit model on history and make a prediction\n",
        "        xyhat = xgboost_forecast(history_xg, testX)\n",
        "        # store forecast in list of predictions\n",
        "        predictions.append(xyhat)\n",
        "        # add actual observation to history for the next loop\n",
        "        history_xg.append(test[j])\n",
        "\n",
        "        xmape= np.mean(np.abs(xyhat - testy)/np.abs(testy))\n",
        "        xrmse= sqrt(np.mean((xyhat-testy)**2))\n",
        "        xd= xyhat-testy\n",
        "        xmae= np.mean(np.abs(xd))\n",
        "\n",
        "    xmapeList.append(xmape)\n",
        "    xrmseList.append(xrmse)\n",
        "    xmaeList.append(xmae)\n",
        "\n",
        "    predictedxg.append(predictions)\n",
        "\n",
        "    # print(\"MAPE: \", xmape)\n",
        "    # print(\"RMSE: \", xrmse)\n",
        "    # print(\"MAE: \", xmae)\n",
        "\n",
        "    i+=1\n",
        "\n",
        "xgb_fit=pd.DataFrame({'predicted': predictedxg})\n",
        "xgb_fit.to_csv('exp_xgb_preds.csv')\n",
        "\n",
        "xgb_metrics=pd.DataFrame({'MAPE' : xmapeList,\n",
        "                    'RMSE': xrmseList,\n",
        "                    'MAE': xmaeList})\n",
        "xgb_metrics.to_csv('exp_xgb_metrics.csv')"
      ],
      "metadata": {
        "id": "awkgZrI5y4Dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "NEURAL NETWORKS\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "H8D3CxnmDVSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !sudo pip3 install keras"
      ],
      "metadata": {
        "id": "wJsTXwjxFSNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from keras.layers import Layer\n",
        "from keras.layers import Bidirectional"
      ],
      "metadata": {
        "id": "n7qYWYhjk8V8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from tensorflow.python.keras.engine.sequential import Sequential\n",
        "from tensorflow.python.keras import layers, callbacks\n",
        "from tensorflow.python.keras.layers import Dense, Conv2D, LSTM, Dropout, GRU, Flatten, Dropout, MaxPooling2D\n",
        "from keras import optimizers"
      ],
      "metadata": {
        "id": "8BPThvYfEOC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import Model\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "KneN5eGQg9gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1234)"
      ],
      "metadata": {
        "id": "gLQhWFnqzAsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2i=df2.iloc[:,0]\n",
        "# df2i=df2i.drop('Year')\n",
        "df2i=df2i.reset_index(drop=True)\n",
        "df2i.head()"
      ],
      "metadata": {
        "id": "PURujA5fCfHT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86c79bfa-a098-45bb-b807-cc1bf38dd4ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    17.365888\n",
              "1    18.333080\n",
              "2    19.184022\n",
              "3    19.069916\n",
              "4    18.506359\n",
              "Name: Australia, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create input dataset\n",
        "def create_dataset (X, look_back = 1):\n",
        "    Xs, ys = [], []\n",
        "\n",
        "    for i in range(len(X)-look_back):\n",
        "        v = X[i:i+look_back]\n",
        "        Xs.append(v)\n",
        "        ys.append(X[i+look_back])\n",
        "\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "# split a univariate sequence into samples\n",
        "def split_sequence(sequence, n_steps):\n",
        "\tX, y = list(), list()\n",
        "\tfor i in range(len(sequence)):\n",
        "\t\t# find the end of this pattern (n_steps is similar to look_back)\n",
        "\t\tend_ix = i + n_steps\n",
        "\t\t# check if we are beyond the sequence\n",
        "\t\tif end_ix > len(sequence)-1:\n",
        "\t\t\tbreak\n",
        "\t\t# gather input and output parts of the pattern\n",
        "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
        "\t\tX.append(seq_x)\n",
        "\t\ty.append(seq_y)\n",
        "\treturn array(X), array(y)"
      ],
      "metadata": {
        "id": "an-IQ8L9Q6We"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "GRU\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "3Nnib079kGTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2len=len(df2.columns)\n",
        "i=0\n",
        "\n",
        "testvals=[]\n",
        "predicted=[]\n",
        "\n",
        "mapeList=[]\n",
        "rmseList=[]\n",
        "maeList=[]\n",
        "\n",
        "while (i <= df2len-1):\n",
        "    dfi=df2.iloc[:,i]\n",
        "    dfi=dfi.reset_index(drop=True)\n",
        "    train=dfi[:24]\n",
        "    test=dfi[-7:]\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    sc_train = scaler.fit_transform(train.values.reshape(-1, 1))\n",
        "    train_scaled = scaler.transform(train.values.reshape(-1, 1))\n",
        "    sc_test = scaler.fit_transform(test.values.reshape(-1,1))\n",
        "    test_scaled = scaler.transform(test.values.reshape(-1,1))\n",
        "\n",
        "    LOOK_BACK = 1\n",
        "    X_train, y_train = create_dataset(train_scaled,LOOK_BACK)\n",
        "    X_test, y_test = create_dataset(test_scaled,LOOK_BACK)\n",
        "    # Print data shape\n",
        "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "    # print(\"X_train.shape:\", X_train.shape)\n",
        "    # print(\"y_train.shape:\", y_train.shape)\n",
        "    # print(\"X_test.shape:\", X_test.shape)\n",
        "    # print(\"y_test.shape:\", y_test.shape)\n",
        "\n",
        "\n",
        "    model = Sequential()\n",
        "    # Input layer\n",
        "    model.add(GRU (units = 200, return_sequences = True,\n",
        "              input_shape = [X_train.shape[1], X_train.shape[2]]))\n",
        "    model.add(Dropout(0.2))\n",
        "    # Hidden layer\n",
        "    model.add(GRU(units = 200))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(units = 1))\n",
        "    #Compile model\n",
        "    model.compile(optimizer=\"adam\",\n",
        "                  loss='mean_absolute_error',\n",
        "                  metrics=['mean_absolute_error'])\n",
        "    # model_gru.summary()\n",
        "\n",
        "    # gru_history = model_gru.fit(X_train, y_train,\n",
        "    #                     epochs=1000,\n",
        "    #                     batch_size=256,\n",
        "    #                     shuffle=False,\n",
        "    #                     validation_split=0.2,\n",
        "    #                     verbose=2)\n",
        "\n",
        "    model.fit(X_train, y_train, epochs=1000, verbose=0)\n",
        "\n",
        "    # predictions\n",
        "    trainPredict = model.predict(X_train)\n",
        "    testPredict = model.predict(X_test)\n",
        "\n",
        "    trainPredict = scaler.inverse_transform(trainPredict)\n",
        "    testPredict  = scaler.inverse_transform(testPredict)\n",
        "    testY        = scaler.inverse_transform(y_test)\n",
        "\n",
        "    # metrics\n",
        "    mape= np.mean(np.abs(testPredict - testY)/np.abs(testY))\n",
        "    rmse= np.mean((testPredict-testY)**2)**.5\n",
        "    d= testPredict-testY\n",
        "    mae= np.mean(abs(d))\n",
        "\n",
        "    predicted.append(testPredict)\n",
        "\n",
        "    # print(\"MAPE: \", mape)\n",
        "    # print(\"RMSE: \", rmse)\n",
        "    # print(\"MAE: \", mae)\n",
        "\n",
        "    mapeList.append(mape)\n",
        "    rmseList.append(rmse)\n",
        "    maeList.append(mae)\n",
        "\n",
        "    i+=1\n",
        "\n",
        "# gru_fit=pd.DataFrame({'predicted': predicted})\n",
        "# gru_fit.to_csv('exp_gru_preds.csv')\n",
        "\n",
        "gru_metricsC=pd.DataFrame({'MAPE' : mapeList,\n",
        "                    'RMSE': rmseList,\n",
        "                    'MAE': maeList})\n",
        "gru_metricsC.to_csv('exp_gru_metricsC.csv')"
      ],
      "metadata": {
        "id": "Y12qr64cGAaA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b889330-eea5-4b0a-a68b-f67f9a981894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 464ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 418ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 413ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 442ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f23c8387e20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 439ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f23b049caf0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 430ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 445ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 427ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 428ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 462ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 413ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 415ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 436ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 447ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 418ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "LSTM\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SvP5J6fjlhkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2len=len(df2.columns)\n",
        "i=0\n",
        "\n",
        "testvals=[]\n",
        "predicted=[]\n",
        "\n",
        "mapeList=[]\n",
        "rmseList=[]\n",
        "maeList=[]\n",
        "\n",
        "while (i <= df2len-1):\n",
        "    dfi=df2.iloc[:,i]\n",
        "    dfi=dfi.reset_index(drop=True)\n",
        "    train=dfi[:24]\n",
        "    test=dfi[-7:]\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    sc_train = scaler.fit_transform(train.values.reshape(-1, 1))\n",
        "    train_scaled = scaler.transform(train.values.reshape(-1, 1))\n",
        "    sc_test = scaler.fit_transform(test.values.reshape(-1,1))\n",
        "    test_scaled = scaler.transform(test.values.reshape(-1,1))\n",
        "\n",
        "    LOOK_BACK = 1\n",
        "    X_train, y_train = create_dataset(train_scaled,LOOK_BACK)\n",
        "    X_test, y_test = create_dataset(test_scaled,LOOK_BACK)\n",
        "    # Print data shape\n",
        "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "    # print(\"X_train.shape:\", X_train.shape)\n",
        "    # print(\"y_train.shape:\", y_train.shape)\n",
        "    # print(\"X_test.shape:\", X_test.shape)\n",
        "    # print(\"y_test.shape:\", y_test.shape)\n",
        "\n",
        "\n",
        "    model_lstm = Sequential()\n",
        "    model_lstm.add(LSTM(200, input_shape=(LOOK_BACK,1),return_sequences = True))\n",
        "    model_lstm.add(LSTM(200, activation='relu',return_sequences = False))\n",
        "    model_lstm.add(Dense(units=1))\n",
        "\n",
        "    # adam = optimizers.Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=None, decay=1e-5, amsgrad=True)\n",
        "    model_lstm.compile(optimizer = 'adam',\n",
        "                  loss='mean_absolute_error',\n",
        "                  metrics=['mean_absolute_error'])\n",
        "    # model_lstm.summary()\n",
        "\n",
        "    # predictions\n",
        "    trainPredict = model_lstm.predict(X_train)\n",
        "    testPredict = model_lstm.predict(X_test)\n",
        "\n",
        "    trainPredict = scaler.inverse_transform(trainPredict)\n",
        "    testPredict  = scaler.inverse_transform(testPredict)\n",
        "    testY        = scaler.inverse_transform(y_test)\n",
        "\n",
        "    # metrics\n",
        "    mape= np.mean(np.abs(testPredict - testY)/np.abs(testY))\n",
        "    rmse= np.mean((testPredict-testY)**2)**.5\n",
        "    d= testPredict-testY\n",
        "    mae= np.mean(abs(d))\n",
        "\n",
        "    predicted.append(testPredict)\n",
        "\n",
        "    # print(\"MAPE: \", mape)\n",
        "    # print(\"RMSE: \", rmse)\n",
        "    # print(\"MAE: \", mae)\n",
        "\n",
        "    mapeList.append(mape)\n",
        "    rmseList.append(rmse)\n",
        "    maeList.append(mae)\n",
        "\n",
        "    i+=1\n",
        "    # print(testPredict)\n",
        "\n",
        "lstm_fit=pd.DataFrame({'predicted': predicted})\n",
        "lstm_fit.to_csv('exp_lstm_preds.csv')\n",
        "\n",
        "lstm_metricsC=pd.DataFrame({'MAPE' : mapeList,\n",
        "                    'RMSE': rmseList,\n",
        "                    'MAE': maeList})\n",
        "lstm_metricsC.to_csv('exp_lstm_metricsC.csv')"
      ],
      "metadata": {
        "id": "jJRHj-NGGVKx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf4bbc2c-f464-4eb1-88f3-2044f2fb8835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 1s 986ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 1s 991ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 1s 982ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 1s 962ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 1s 950ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 1s 967ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 1s 956ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 30ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "CNN\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "seTnQrfxhio0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "\n",
        "df2len=len(df2.columns)\n",
        "i=0\n",
        "\n",
        "testvals=[]\n",
        "predicted=[]\n",
        "\n",
        "mapeList=[]\n",
        "rmseList=[]\n",
        "maeList=[]\n",
        "\n",
        "while (i <= df2len-1):\n",
        "    dfi=df2.iloc[:,i]\n",
        "    dfi=dfi.reset_index(drop=True)\n",
        "    train=dfi[:24]\n",
        "    seq=train.values\n",
        "    test=dfi[-7:]\n",
        "    input=test.values\n",
        "\n",
        "    # choose a number of time steps\n",
        "    n_steps = 1\n",
        "    # split into samples\n",
        "    X_train, y_train = split_sequence(seq, n_steps)\n",
        "    X_test, y_test= split_sequence(input, n_steps)\n",
        "    # reshape from [samples, timesteps] into [samples, timesteps, features]\n",
        "    n_features = 1\n",
        "    # Print data shape\n",
        "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "    # define model\n",
        "    model_cnn = Sequential()\n",
        "    model_cnn.add(Conv1D(filters=200, kernel_size=1, activation='relu', input_shape=(n_steps,1)))\n",
        "    model_cnn.add(MaxPooling1D(pool_size=1))\n",
        "    model_cnn.add(Flatten())\n",
        "    model_cnn.add(Dense(50, activation='relu'))\n",
        "    model_cnn.add(Dense(1))\n",
        "    model_cnn.compile(optimizer = 'adam',\n",
        "                      loss='mean_absolute_error',\n",
        "                      metrics=['mean_absolute_error'])\n",
        "    # # fit model\n",
        "    model_cnn.fit(X_train, y_train, epochs=1000, verbose=0)\n",
        "    # Predictions\n",
        "    trainPredict = model_cnn.predict(X_train)\n",
        "    testPredict = model_cnn.predict(X_test)\n",
        "\n",
        "    mape= np.mean(np.abs(testPredict - y_test)/np.abs(y_test))\n",
        "    rmse= np.mean((testPredict-y_test)**2)**.5\n",
        "    d= testPredict-y_test\n",
        "    mae= np.mean(abs(d))\n",
        "\n",
        "    predicted.append(testPredict)\n",
        "\n",
        "    # print(\"MAPE: \", mape)\n",
        "    # print(\"RMSE: \", rmse)\n",
        "    # print(\"MAE: \", mae)\n",
        "\n",
        "    mapeList.append(mape)\n",
        "    rmseList.append(rmse)\n",
        "    maeList.append(mae)\n",
        "\n",
        "    i+=1\n",
        "\n",
        "# cnn_fit=pd.DataFrame({'predicted': predicted})\n",
        "# cnn_fit.to_csv('exp_cnn_preds.csv')\n",
        "\n",
        "cnn_metricsC=pd.DataFrame({'MAPE' : mapeList,\n",
        "                    'RMSE': rmseList,\n",
        "                    'MAE': maeList})\n",
        "cnn_metricsC.to_csv('exp_cnn_metricsC.csv')"
      ],
      "metadata": {
        "id": "FZiSWoEzOxJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "BI-DIR RNN\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "czgvdFh4gAej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from keras.layers import Bidirectional\n",
        "# from tensorflow import keras\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import LSTM\n",
        "# from keras.layers import Activation, Dense\n",
        "# import numpy as np"
      ],
      "metadata": {
        "id": "Fv2lMZDiq-gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df2len=len(df2.columns)\n",
        "# i=0\n",
        "\n",
        "# testvals=[]\n",
        "# predicted=[]\n",
        "\n",
        "# mapeList=[]\n",
        "# rmseList=[]\n",
        "# maeList=[]\n",
        "\n",
        "# while (i <= df2len-1):\n",
        "#     dfi=df2.iloc[:,i]\n",
        "#     dfi=dfi.reset_index(drop=True)\n",
        "#     train=dfi[:24]\n",
        "#     test=dfi[-7:]\n",
        "\n",
        "#     scaler = MinMaxScaler()\n",
        "#     sc_train = scaler.fit_transform(train.values.reshape(-1, 1))\n",
        "#     train_scaled = scaler.transform(train.values.reshape(-1, 1))\n",
        "#     sc_test = scaler.fit_transform(test.values.reshape(-1,1))\n",
        "#     test_scaled = scaler.transform(test.values.reshape(-1,1))\n",
        "\n",
        "#     LOOK_BACK = 1\n",
        "#     X_train, y_train = create_dataset(train_scaled,LOOK_BACK)\n",
        "#     X_test, y_test = create_dataset(test_scaled,LOOK_BACK)\n",
        "#     # Print data shape\n",
        "#     X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "#     X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "#     # print(\"X_train.shape:\", X_train.shape)\n",
        "#     # print(\"y_train.shape:\", y_train.shape)\n",
        "#     # print(\"X_test.shape:\", X_test.shape)\n",
        "#     # print(\"y_test.shape:\", y_test.shape)\n",
        "\n",
        "#     model_bi = Sequential()\n",
        "#     model_bi.add(Bidirectional(LSTM(32, return_sequences=True), input_shape=(1, 1)))\n",
        "#     model_bi.add(Bidirectional(LSTM(32)))\n",
        "#     model_bi.add(Activation('relu'))\n",
        "#     model_bi.compile(loss='categorical_crossentropy', optimizer='Adam')\n",
        "\n",
        "#     # predictions\n",
        "#     trainPredict = model_bi.predict(X_train)\n",
        "#     testPredict = model_bi.predict(X_test)\n",
        "\n",
        "#     trainPredict = scaler.inverse_transform(trainPredict)\n",
        "#     testPredict  = scaler.inverse_transform(testPredict)\n",
        "#     testY        = scaler.inverse_transform(y_test)\n",
        "\n",
        "#     # metrics\n",
        "#     mape= np.mean(np.abs(testPredict - testY)/np.abs(testY))\n",
        "#     rmse= np.mean((testPredict-testY)**2)**.5\n",
        "#     d= testPredict-testY\n",
        "#     mae= np.mean(abs(d))\n",
        "\n",
        "#     predicted.append(testPredict)\n",
        "\n",
        "#     # print(\"MAPE: \", mape)\n",
        "#     # print(\"RMSE: \", rmse)\n",
        "#     # print(\"MAE: \", mae)\n",
        "\n",
        "#     mapeList.append(mape)\n",
        "#     rmseList.append(rmse)\n",
        "#     maeList.append(mae)\n",
        "\n",
        "#     i+=1\n",
        "#     # print(testPredict)\n",
        "\n",
        "# biRnn_fit=pd.DataFrame({'predicted': predicted})\n",
        "# biRnn_fit.to_csv('gvt_biRnn_metrics.csv')\n",
        "\n",
        "# biRnn_metrics=pd.DataFrame({'MAPE' : mapeList,\n",
        "#                     'RMSE': rmseList,\n",
        "#                     'MAE': maeList})\n",
        "# biRnn_metrics.to_csv('gvt_biRnn_metrics.csv')\n"
      ],
      "metadata": {
        "id": "kRmph_CLgANa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "ATT RNN\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "gUQSZGXagEzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from keras.layers import Input, Dense, SimpleRNN\n",
        "\n",
        "# # Create a traditional RNN network\n",
        "# def create_RNN(hidden_units, dense_units, input_shape, activation):\n",
        "#     model = Sequential()\n",
        "#     model.add(SimpleRNN(hidden_units, input_shape=input_shape, activation=activation[0]))\n",
        "#     model.add(Dense(units=dense_units, activation=activation[1]))\n",
        "#     model.compile(loss='mse', optimizer='adam')\n",
        "#     return model\n",
        "# df2len=len(df2.columns)\n",
        "# i=0\n",
        "\n",
        "# testvals=[]\n",
        "# predicted=[]\n",
        "\n",
        "# mapeList=[]\n",
        "# rmseList=[]\n",
        "# maeList=[]\n",
        "\n",
        "# while (i <= df2len-1):\n",
        "#     dfi=df2.iloc[:,i]\n",
        "#     dfi=dfi.reset_index(drop=True)\n",
        "#     train=dfi[:24]\n",
        "#     test=dfi[-7:]\n",
        "\n",
        "#     scaler = MinMaxScaler()\n",
        "#     sc_train = scaler.fit_transform(train.values.reshape(-1, 1))\n",
        "#     train_scaled = scaler.transform(train.values.reshape(-1, 1))\n",
        "#     sc_test = scaler.fit_transform(test.values.reshape(-1,1))\n",
        "#     test_scaled = scaler.transform(test.values.reshape(-1,1))\n",
        "\n",
        "#     LOOK_BACK = 1\n",
        "#     X_train, y_train = create_dataset(train_scaled,LOOK_BACK)\n",
        "#     X_test, y_test = create_dataset(test_scaled,LOOK_BACK)\n",
        "#     # Print data shape\n",
        "#     X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "#     X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "#     # print(\"X_train.shape:\", X_train.shape)\n",
        "#     # print(\"y_train.shape:\", y_train.shape)\n",
        "#     # print(\"X_test.shape:\", X_test.shape)\n",
        "#     # print(\"y_test.shape:\", y_test.shape)\n",
        "\n",
        "#     # Set up parameters\n",
        "#     # time_steps = 20\n",
        "#     hidden_units = 2\n",
        "#     epochs = 30\n",
        "#     model_RNN = create_RNN(hidden_units=hidden_units, dense_units=1, input_shape=(1,1),\n",
        "#                    activation=['tanh', 'tanh'])\n",
        "#     # model_RNN.summary()\n",
        "#     # predictions\n",
        "#     trainPredict = model_RNN.predict(X_train)\n",
        "#     testPredict = model_RNN.predict(X_test)\n",
        "\n",
        "#     trainPredict = scaler.inverse_transform(trainPredict)\n",
        "#     testPredict  = scaler.inverse_transform(testPredict)\n",
        "#     testY        = scaler.inverse_transform(y_test)\n",
        "\n",
        "#     # metrics\n",
        "#     mape= np.mean(np.abs(testPredict - testY)/np.abs(testY))\n",
        "#     rmse= np.mean((testPredict-testY)**2)**.5\n",
        "#     d= testPredict-testY\n",
        "#     mae= np.mean(abs(d))\n",
        "\n",
        "#     predicted.append(testPredict)\n",
        "\n",
        "#     # print(\"MAPE: \", mape)\n",
        "#     # print(\"RMSE: \", rmse)\n",
        "#     # print(\"MAE: \", mae)\n",
        "\n",
        "#     mapeList.append(mape)\n",
        "#     rmseList.append(rmse)\n",
        "#     maeList.append(mae)\n",
        "\n",
        "#     i+=1\n",
        "#     # print(testPredict)\n",
        "\n",
        "# Rnn_fit=pd.DataFrame({'predicted': predicted})\n",
        "# Rnn_fit.to_csv('gvt_Rnn_metrics.csv')\n",
        "\n",
        "# Rnn_metrics=pd.DataFrame({'MAPE' : mapeList,\n",
        "#                     'RMSE': rmseList,\n",
        "#                     'MAE': maeList})\n",
        "# Rnn_metrics.to_csv('gvt_Rnn_metrics.csv')"
      ],
      "metadata": {
        "id": "Y9FY6O32fX9D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}